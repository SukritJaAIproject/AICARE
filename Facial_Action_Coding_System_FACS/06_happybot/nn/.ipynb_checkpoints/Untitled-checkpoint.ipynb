{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import facs_helper\n",
    "import facs_preparation\n",
    "from tqdm import tqdm\n",
    "\n",
    "#%% INITIALIZE VARIABLES\n",
    "###################################################################\n",
    "\n",
    "facsList = [  1.,   2.,   4.,   5.,   6.,   7.,   9.,  10.,  11.,  12.,  13.,\n",
    "        14.,  15.,  16.,  17.,  18.,  20.,  21.,  22.,  23.,  24.,  25.,\n",
    "        26.,  27.,  28.,  29.,  30.,  31.,  34.,  38.,  39.,  43.,  44.,\n",
    "        45.,  54.,  61.,  62.,  63.,  64.]\n",
    "facsPaper = [5., 9., 12., 15., 16.,20.,23., 24.] # Kotsia FAU\n",
    "facsTop = [1.,2.,4.,5.,6.,7.]\n",
    "\n",
    "facsBtm = np.array([9., 10., 12., 15., 17., 20., 23.,24., 25., 26., 27.])\n",
    "# get rid of facs 24, 26, 23, 10\n",
    "\n",
    "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "faces_folder_path = 'C:/Users/sukri/Desktop/CK_dataset/CK+/cohn-kanade-images/**/*/'\n",
    "#landmark_path = '/Users/joshualamstein/Desktop/CK+/Landmarks/S022/003/S022_003_00000005_landmarks.txt'\n",
    "facs_folder = 'C:/Users/sukri/Desktop/CK_dataset/CK+/FACS/**/*/'\n",
    "emotion_folder = 'C:/Users/sukri/Desktop/CK_dataset/CK+/Emotion/'\n",
    "\n",
    "# Boolean to select whether you want to train for top FACS, bottom FACS, or facs from Kotsia's paper. \n",
    "paperBool = False\n",
    "topBool = True\n",
    "btmBool = True\n",
    "\n",
    "#%% FACIAL FEATURES\n",
    "\n",
    "# Get changes in distance of key facial features such as eyebrow distance or distance between\n",
    "# parted lips. \n",
    "facial_features = facs_preparation.features(faces_folder_path, predictor_path)\n",
    "facial_features.get_features()\n",
    "facialMotion = facial_features.facialMotion \n",
    "facialMotionLower = facial_features.facialMotionLower \n",
    "\n",
    "#%% FACS\n",
    "###################################################################\n",
    "\n",
    "# Gets AUs of facial sequences\n",
    "folder = facs_helper.facsFolder(facs_folder, facsList, facsTop, facsBtm, facsPaper)\n",
    "folder.process()\n",
    "\n",
    "inten = folder.inten \n",
    "intenT = folder.intenT\n",
    "intenB = folder.intenB\n",
    "facs= folder.facs\n",
    "facsT = folder.facsT\n",
    "facsB= folder.facsB\n",
    "facsPath = folder.facsPath\n",
    "\n",
    "print('Finished facs')\n",
    "\n",
    "#%% BASIC EMOTIONS\n",
    "\"\"\"\n",
    "This class sees how well the ground truth AUs predict the ground truth emotions. They have a 76% match.\n",
    "For the classification, I referred to Facial Expression Recognition in Image Sequences Using Geometric Deformation Features\n",
    "and Support Vector Machines by Kotsia and https://en.wikipedia.org/wiki/Facial_Action_Coding_System.\n",
    "\n",
    "In the CK+ dataset, the labels for emotions are:\n",
    "Basic Emotions: 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise\n",
    "\n",
    "  Basic emotions and their associated AUs:\n",
    "  \n",
    "  # Happiness - AU 6, 12\n",
    "  # Sadness - AU 1,4,15\n",
    "  # Surprise - AU 1,2,5,26\n",
    "  # Fear - AU 1,2,4,5,7,20,26\n",
    "  # Anger - AU 4,5,7,23\n",
    "  # Disgust - AU 9,15,16 \n",
    "  # Contempt - AU 12,14\n",
    "  \n",
    "  emotions: emotions associated with sequence\n",
    "  emotionsPath: path to emotions\n",
    "  emoMatch: Returns the emotion number if matched, -1 if no match\n",
    "  matchBool: True for match, false for no\n",
    "  countM, E: counts the different emotions that matched\n",
    "  match_miss_count: counts which predictions didn't match\n",
    "  emo_miss_count: counts which predictions didn't match in ground truth\n",
    "  \n",
    "\"\"\"\n",
    "###################################################################\n",
    "\n",
    "# Get emotion of samples. Emotions may be found after AUs are predicted. \n",
    "emo = facs_helper.emoFolder(emotion_folder, facs_folder)\n",
    "emo.process()\n",
    "emotions = emo.emotions\n",
    "emotionPath = emo.emotionPath\n",
    "\n",
    "comp = facs_helper.compare(emotions,facsT, facsB)\n",
    "emoMatch, matchBool = comp.match()\n",
    "\n",
    "# Using the true values of AUs and basic emotions, \n",
    "# check the accuracy of how the AUs determine emotions. \n",
    "# Happiness is very clear based on AUs, but anger and \n",
    "# disgust have similar AUs and they can be confused. \n",
    "uniqueM, countM = np.unique(emoMatch, return_counts=True)\n",
    "uniqueE, countE = np.unique(emotions,return_counts=True)\n",
    "emoMatch[emoMatch==-2]=-1\n",
    "# This checks where the the predicted matches against the ground truth emotions.\n",
    "ii = np.where(emoMatch != emotions)\n",
    "\n",
    "emoM,match_miss_count = np.unique(emoMatch[ii],return_counts=True)\n",
    "emoList,emo_miss_count = np.unique(emotions[ii],return_counts=True)\n",
    "\n",
    "\n",
    "#%% Set up data\n",
    "###################################################################\n",
    "\n",
    "# Write 'top' or 'bot' to select whether the model should train\n",
    "# for AUs in the top of the face or the bottom of the face. \n",
    "# Mind that you have had topBool and botBool properly set when you processed data!\n",
    "selectData = 'top'\n",
    "\n",
    "if selectData == 'top':\n",
    "    facialSelect = facialMotion\n",
    "    # You can select which top AUs you train for with facsLimit. \n",
    "    facsLimit = 5\n",
    "    facsLevel = facsT[:,:facsLimit]\n",
    "    facsUse = facsTop[:facsLimit]\n",
    "    weight_inten = intenT[:,:facsLimit]\n",
    "\n",
    "elif selectData == 'bot':\n",
    "\n",
    "    useBotIdx = np.array([0,2,3,4,5,8,10], dtype = np.int32) # You can select which AUs you train for with this array. \n",
    "    facialSelect = facialMotionLower\n",
    "    facsLevel = facsB[:,useBotIdx]\n",
    "    facsUse = facsBtm[useBotIdx]\n",
    "    weight_inten= intenB[:,useBotIdx]\n",
    "\n",
    "# Randomize samples\n",
    "train_x, test_x, train_y, test_y, sw_train,sw_test= train_test_split(\n",
    "    facialSelect, facsLevel, weight_inten, test_size=0.25, random_state=42) #0.25, 42\n",
    "\n",
    "numFacs = len(facsUse)\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_shape = facialSelect.shape\n",
    "output_shape = [facialSelect.shape[0],numFacs]\n",
    "num_samples = facialSelect.shape[0]\n",
    "n_input = facialSelect.shape[1]\n",
    "n_hidden_1 = 256 #256 neurons\n",
    "n_hidden_2 = 256\n",
    "n_hidden_3 = 256\n",
    "n_hidden_4 = 256\n",
    "n_hidden_5 = 256\n",
    "n_hidden_6 = 256\n",
    "n_output = numFacs\n",
    "\n",
    "#%% Feed forward MLP NN\n",
    "###################################################################\n",
    "\n",
    "LOGDIR = \"/51/\"\n",
    "savepath = \"/save_tf_upper_zappa_11/\"\n",
    "\n",
    "\n",
    "# Function for fully connected layer\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        print(\"w\",w.dtype)\n",
    "        print(\"b\",b.dtype)\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "   \n",
    "# Neural Network Model\n",
    "def facs_model(learning_rate, scale_class_weight,use_two_fc, use_three_fc, use_four_fc, use_five_fc, use_six_fc, use_seven_fc, hparam):\n",
    "    config = tf.ConfigProto(graph_options=tf.GraphOptions(\n",
    "    optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session(\"\", config = config)\n",
    "\n",
    "  # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, [None, n_input], name = \"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, n_output], name = \"labels\")\n",
    "    sw = tf.placeholder(tf.float32, [None, n_output], name = 'intensity_weights')\n",
    "\n",
    "    # Main function compares the number of FCs for performance. \n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc1/relu\", relu)\n",
    "        logits = fc_layer(relu, n_hidden_1, n_output, \"fc2\")\n",
    "    elif use_three_fc: \n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc3/relu\", relu)\n",
    "        fc2 = fc_layer(relu,n_hidden_1,n_hidden_2,\"fc2\")\n",
    "        relu_2 = tf.nn.relu(fc2)\n",
    "        tf.summary.histogram(\"fc3/relu\", relu_2)\n",
    "        logits = fc_layer(relu_2, n_hidden_2, n_output, \"fc3\")\n",
    "    elif use_four_fc: \n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc4/relu\", relu)\n",
    "        fc2 = fc_layer(relu,n_hidden_1,n_hidden_2,\"fc2\")\n",
    "        relu_2 = tf.nn.relu(fc2)\n",
    "        tf.summary.histogram(\"fc4/relu\", relu_2)\n",
    "        fc3 = fc_layer(relu_2,n_hidden_2,n_hidden_3,\"fc3\")\n",
    "        relu_3 = tf.nn.relu(fc3)\n",
    "        tf.summary.histogram(\"fc4/relu\", relu_3)\n",
    "        logits = fc_layer(relu_3, n_hidden_3, n_output, \"fc4\")\n",
    "    elif use_five_fc: \n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc5/relu\", relu)\n",
    "        fc2 = fc_layer(relu,n_hidden_1,n_hidden_2,\"fc2\")\n",
    "        relu_2 = tf.nn.relu(fc2)\n",
    "        tf.summary.histogram(\"fc5/relu\", relu_2)\n",
    "        fc3 = fc_layer(relu_2,n_hidden_2,n_hidden_3,\"fc3\")\n",
    "        relu_3 = tf.nn.relu(fc3)\n",
    "        tf.summary.histogram(\"fc5/relu\", relu_3)\n",
    "        fc4 = fc_layer(relu_3,n_hidden_3,n_hidden_4,\"fc4\")\n",
    "        relu_4 = tf.nn.relu(fc4)\n",
    "        tf.summary.histogram(\"fc5/relu\", relu_4)\n",
    "        logits = fc_layer(relu_4, n_hidden_4, n_output, \"fc5\")\n",
    "    elif use_six_fc: \n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc6/relu\", relu)\n",
    "        fc2 = fc_layer(relu,n_hidden_1,n_hidden_2,\"fc2\")\n",
    "        relu_2 = tf.nn.relu(fc2)\n",
    "        tf.summary.histogram(\"fc6/relu\", relu_2)\n",
    "        fc3 = fc_layer(relu_2,n_hidden_2,n_hidden_3,\"fc3\")\n",
    "        relu_3 = tf.nn.relu(fc3)\n",
    "        tf.summary.histogram(\"fc6/relu\", relu_3)\n",
    "        fc4 = fc_layer(relu_3,n_hidden_3,n_hidden_4,\"fc4\")\n",
    "        relu_4 = tf.nn.relu(fc4)\n",
    "        tf.summary.histogram(\"fc6/relu\", relu_4)\n",
    "        fc5 = fc_layer(relu_4,n_hidden_4,n_hidden_5,\"fc5\")\n",
    "        relu_5 = tf.nn.relu(fc5)\n",
    "        tf.summary.histogram(\"fc6/relu\", relu_5)\n",
    "        logits = fc_layer(relu_5, n_hidden_5, n_output, \"fc6\")\n",
    "    elif use_seven_fc: \n",
    "        fc1 = fc_layer(x, n_input, n_hidden_1, \"fc1\")\n",
    "        relu = tf.nn.relu(fc1)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu)\n",
    "        fc2 = fc_layer(relu,n_hidden_1,n_hidden_2,\"fc2\")\n",
    "        relu_2 = tf.nn.relu(fc2)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu_2)\n",
    "        fc3 = fc_layer(relu_2,n_hidden_2,n_hidden_3,\"fc3\")\n",
    "        relu_3 = tf.nn.relu(fc3)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu_3)\n",
    "        fc4 = fc_layer(relu_3,n_hidden_3,n_hidden_4,\"fc4\")\n",
    "        relu_4 = tf.nn.relu(fc4)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu_4)\n",
    "        fc5 = fc_layer(relu_4,n_hidden_4,n_hidden_5,\"fc5\")\n",
    "        relu_5 = tf.nn.relu(fc5)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu_5)\n",
    "        fc6 = fc_layer(relu_5,n_hidden_5,n_hidden_6,\"fc6\")\n",
    "        relu_6 = tf.nn.relu(fc6)\n",
    "        tf.summary.histogram(\"fc7/relu\", relu_6)\n",
    "        logits = fc_layer(relu_6, n_hidden_6, n_output, \"fc7\")\n",
    "    \n",
    "    else:\n",
    "        logits = fc_layer(x, n_input, n_output, \"fc\")\n",
    "\n",
    "    # Loss function \n",
    "    with tf.name_scope(\"xent\"):\n",
    "        # The positive and negative samples in the data are unbalanced. \n",
    "        # To push the algorithm to focus on fitting positives, I weighted the \n",
    "        # positive values more than the negative.\n",
    "    \n",
    "        maxY = tf.reduce_sum(y,1)* scale_class_weight\n",
    "        class_weights = (maxY + 1)/6\n",
    "        \n",
    "        # Some expressions are more intense than others in the CK+ database and \n",
    "        # and that is weighted in the loss function by sample weights, sw. \n",
    "        # However, I got better results with just weighting all AUs\n",
    "        # with equal intensity. \n",
    "\n",
    "#        mult_w = tf.multiply(y, sw) \n",
    "#        sum_w = tf.reduce_sum(mult_w,1)\n",
    "#        \n",
    "#        class_weights = ( sum_w + 1) / 6\n",
    "        \n",
    "        print(class_weights.get_shape())\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y, name=\"xent\"))\n",
    "        xent = tf.reduce_mean(xent * class_weights)\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        zero = tf.constant(0, dtype=tf.float32)\n",
    "    \n",
    "        onesMat = tf.ones_like(logits)\n",
    "        zerosMat = tf.zeros_like(logits)\n",
    "        onesY = tf.ones_like(y,dtype=tf.float32)\n",
    "        \n",
    "        yFloat = tf.cast(y,dtype = tf.float32)    \n",
    "        yFlipped = onesY - yFloat\n",
    "        # PREDICTION - If logits >= 0, logits = 1, else logits = 0. \n",
    "        logitsBin = tf.cast(tf.where(logits>=zero,onesMat,zerosMat),dtype=tf.float32, name = \"op_to_restore\")\n",
    "        \n",
    "        \n",
    "        tf.add_to_collection(\"coll\", logitsBin)\n",
    "        tf.add_to_collection(\"coll\", x)\n",
    "        \n",
    "    \n",
    "        print('logitsBin', logitsBin.get_shape())\n",
    "        print('y', y.get_shape())\n",
    "        print('where_logitsBin', tf.where(logitsBin)[:,1].get_shape())\n",
    "        print('where_y', tf.where(y)[:,1].get_shape())\n",
    "        time_steps = tf.cast(tf.shape(y)[0], dtype = 'int32')\n",
    "        print(time_steps.get_shape())\n",
    "    \n",
    "        nFacs = tf.count_nonzero(y,1,dtype=tf.float32)\n",
    "        onesFacs = tf.ones_like(nFacs)\n",
    "        nFacs_Zeros = onesFacs*numFacs - nFacs\n",
    "    \n",
    "        nFacs = tf.where(tf.equal(nFacs,zero), onesFacs, nFacs)\n",
    "        nFacs_Zeros = tf.where(tf.equal(nFacs_Zeros,zero), onesFacs, nFacs_Zeros)\n",
    "    \n",
    "        # Find TPR, TNR, FPR, FNR. \n",
    "        matrix_positive = tf.cast(tf.equal(logitsBin, y) & tf.equal(yFloat, tf.constant(1,dtype=tf.float32)),dtype=tf.float32)\n",
    "        correct_pos = tf.reduce_sum(matrix_positive) / tf.reduce_sum(yFloat)\n",
    "        tf.summary.scalar(\"TruePosRate\", correct_pos)\n",
    "        \n",
    "        matrix_negative = tf.cast(tf.equal(logitsBin, y) & tf.equal(yFloat, zero),dtype = tf.float32)\n",
    "        correct_neg = tf.reduce_sum(matrix_negative) / tf.reduce_sum(yFlipped)\n",
    "        tf.summary.scalar(\"TrueNegRate\", correct_neg)\n",
    "        \n",
    "        matrix_falsePos = tf.cast(tf.not_equal(logitsBin, y) & tf.equal(y, zero),dtype = tf.float32) #or yFlipped = 1\n",
    "        falsePos = tf.reduce_sum(matrix_falsePos) / tf.reduce_sum(yFlipped)\n",
    "        tf.summary.scalar(\"falsePosRate\", falsePos)\n",
    "        \n",
    "        matrix_falseNeg = tf.cast(tf.not_equal(logitsBin, y) & tf.equal(yFloat, tf.constant(1,dtype=tf.float32)),dtype = tf.float32)\n",
    "        falseNeg = tf.reduce_sum(matrix_falseNeg) / tf.reduce_sum(yFloat)\n",
    "        tf.summary.scalar(\"falseNegRate\", falseNeg)  \n",
    "        \n",
    "        \n",
    "        tp_sum = tf.reduce_sum(matrix_positive,0)\n",
    "        tp_sum_append = tf.concat([tf.constant([0],dtype = tf.float32), tp_sum],0)\n",
    "        tf_sum = tf.reduce_sum(matrix_negative,0)\n",
    "        fp_sum = tf.reduce_sum(matrix_falsePos,0)\n",
    "        fn_sum = tf.reduce_sum(matrix_falseNeg,0)\n",
    "\n",
    "        # Get Matrix of Confusion for multiclass binary classifier. \n",
    "        confusion = tf.Variable(initial_value = tf.zeros([n_output+1,n_output+1]), name = 'confusion')\n",
    "        confusion1 = tf.Variable(initial_value = tf.cast( tf.diag(np.repeat(1,n_output+1)),dtype = tf.float32), name = 'confusion1')\n",
    "        confusion2 = tf.Variable(initial_value = tf.zeros([n_output+1,n_output+1]), name = 'confusion2')\n",
    "        confusion3 = tf.Variable(initial_value = tf.zeros([n_output+1,n_output+1]), name = 'confusion3')\n",
    "        confusion4 = tf.Variable(initial_value = tf.zeros([n_output+1,n_output+1]), name = 'confusion4')\n",
    "\n",
    "        confusion1 = confusion1[0,0].assign(5)\n",
    "        confusion1= confusion1 * tp_sum_append\n",
    "        confusion2 = confusion2[0,0].assign(tf.reduce_sum(tf_sum))\n",
    "        confusion3 = tf.assign(confusion3[0,1:n_output+1],fp_sum)\n",
    "        confusion4 = confusion4[1:n_output+1,0].assign(fn_sum)\n",
    "        \n",
    "        confusion = confusion1 + confusion2 + confusion3 +confusion4\n",
    "    \n",
    "        txtConfusion = tf.as_string(confusion,precision = 0, name='txtConfusion')\n",
    "    \n",
    "        tf.summary.text('txtConfusion', txtConfusion)\n",
    "        \n",
    "        correct_prediction = tf.cast(tf.equal(logitsBin,y),dtype = tf.float32, name = \"correct_prediction\")\n",
    "            \n",
    "        accuracy = tf.reduce_mean(correct_prediction, name = \"accuracy\")\n",
    "        \n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "\n",
    "# Summary for tensorboard\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    \n",
    "  \n",
    "    writer = tf.summary.FileWriter(LOGDIR + hparam + '/train')\n",
    "    test_writer = tf.summary.FileWriter(LOGDIR + hparam + '/test')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(3001)):\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: train_x, y: train_y, sw:sw_train})\n",
    "            sess.run([confusion],feed_dict={x: test_x, y: test_y, sw: sw_test})\n",
    "\n",
    "\n",
    "            writer.add_summary(s, i)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            [acc,s] = sess.run([accuracy, summ],feed_dict={x: test_x, y: test_y, sw: sw_test})\n",
    "            sess.run([confusion],feed_dict={x: test_x, y: test_y, sw:sw_test})\n",
    "            test_writer.add_summary(s,i)\n",
    "            saver.save(sess, os.path.join(savepath,hparam, \"model\"), i)\n",
    "        sess.run(train_step, feed_dict={x: train_x, y: train_y, sw: sw_train})\n",
    "    \n",
    "def make_hparam_string(learning_rate, scale_class_weight,use_two_fc, use_three_fc, use_four_fc, use_five_fc, use_six_fc, use_seven_fc):\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"\"\n",
    "    fc_param_3 = \"fc=3\" if use_three_fc else \"\"\n",
    "    fc_param_4 = \"fc=4\" if use_four_fc else \"\"\n",
    "    fc_param_5 = \"fc=5\" if use_five_fc else \"\"\n",
    "    fc_param_6 = \"fc=6\" if use_six_fc else \"\"\n",
    "    fc_param_7 = \"fc=7\" if use_seven_fc else \"\"\n",
    "    return \"lr_%.0E,cw_%i,%s%s%s%s%s%s\" % (learning_rate, scale_class_weight, fc_param, fc_param_3, fc_param_4, fc_param_5, fc_param_6, fc_param_7)\n",
    "\n",
    "def main():\n",
    "  # Testing learning rates\n",
    "    for learning_rate in tqdm([1E-2, 1E-3, 1E-4]):\n",
    "        for scale_class_weight in [1]:\n",
    "\n",
    "    # Include \"False\" as a value to try different model architectures\n",
    "            for p in range(0,6):\n",
    "                for use_two_fc in [p==0]:\n",
    "                    for use_three_fc in [p==1]:\n",
    "                        for use_four_fc in [p==2]:\n",
    "                            for use_five_fc in [p==3]:\n",
    "                                for use_six_fc in [p==4]:\n",
    "                                    for use_seven_fc in [p==5]:\n",
    "            # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2\")\n",
    "                                        hparam = make_hparam_string(learning_rate,scale_class_weight, use_two_fc, use_three_fc, use_four_fc, use_five_fc, use_six_fc, use_seven_fc)\n",
    "                                        print('Starting run for %s' % hparam)\n",
    "                                \n",
    "                                        # Run\n",
    "                                        facs_model(learning_rate, scale_class_weight,use_two_fc, use_three_fc, use_four_fc, use_five_fc, use_six_fc, use_seven_fc, hparam)\n",
    "    print('Done training!')\n",
    "    print('Run `tensorboard --logdir=%s` to see the results.' % LOGDIR)\n",
    "    print('Running on mac? If you want to get rid of the dialogue asking to give '\n",
    "    'network permissions to TensorBoard, you can provide this flag: '\n",
    "    '--host=localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-matthew",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
