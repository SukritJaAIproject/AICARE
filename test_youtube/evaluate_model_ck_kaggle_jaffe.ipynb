{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_model_ck_kaggle_jaffe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3647d4e5ef794145897ed45d83204389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_235cbea959fb4ef89b042dfcd77bc8f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_073b46307b9e4c5a97292293ca0828ce",
              "IPY_MODEL_0b609aad7d4d469892a0022fe25caf06"
            ]
          }
        },
        "235cbea959fb4ef89b042dfcd77bc8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "073b46307b9e4c5a97292293ca0828ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b009c708c514eb5850cf2fcac30f365",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 66,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 66,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed11d1b10a4d4a87920db69847425930"
          }
        },
        "0b609aad7d4d469892a0022fe25caf06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_917ccd063364453ebe7a0b5c8bdce712",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 66/66 [7:12:01&lt;00:00, 392.76s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e271a4bbce1c4038ac0b10a259df3372"
          }
        },
        "9b009c708c514eb5850cf2fcac30f365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed11d1b10a4d4a87920db69847425930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "917ccd063364453ebe7a0b5c8bdce712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e271a4bbce1c4038ac0b10a259df3372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPFVCHjcQrEv",
        "outputId": "cd25e214-a8c9-437d-be47-f7d1a766d75e"
      },
      "source": [
        "!pip install -q py-feat\n",
        "# !pip install -q facial-emotion-recognition\n",
        "# !pip install -q fer\n",
        "# !pip install -q deepface\n",
        "# !pip install -q mtcnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.0MB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 57.4MB 46kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9MB 34.1MB/s \n",
            "\u001b[?25h  Building wheel for py-feat (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpu_xoUpRHxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c13afc-81c9-4d20-ab4d-6c93a9000bb1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from feat import Detector\n",
        "import glob\n",
        "import math\n",
        "import dlib\n",
        "import joblib\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras import optimizers \n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
        "\n",
        "face_model = \"RetinaFace\"\n",
        "landmark_model = \"MobileNet\"\n",
        "au_model = \"svm\"\n",
        "emotion_model = \"resmasknet\" #resmasknet,fer, svm, rf\n",
        "detector = Detector(face_model = face_model, landmark_model = landmark_model, au_model = au_model, emotion_model = emotion_model)\n",
        "\n",
        "def my_mode(sample):\n",
        "  c = Counter(sample)\n",
        "  return [k for k, v in c.items() if v == c.most_common(1)[0][1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUrNBJdNRUJ8"
      },
      "source": [
        "## ALL STEP IN ONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knSi6HEAeUaC"
      },
      "source": [
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Create  model\n",
        "model_4 = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "base_model_layers = model_4.layers[:23]\n",
        "new_model_layers =  model_4.layers[23:]\n",
        "\n",
        "base_model_blocks = {\n",
        "    0: base_model_layers[1:3],\n",
        "    1: base_model_layers[4:6],\n",
        "    2: base_model_layers[7:11],\n",
        "    3: base_model_layers[12:16],\n",
        "    4: base_model_layers[17:21]\n",
        "}\n",
        "\n",
        "# Load the best model\n",
        "model_4_path = \"./drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/model_ck_kaggle_jaffe/{}.h5\".format(\"model_4\")\n",
        "model_4.load_weights(model_4_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhlYnFD5T2Vm"
      },
      "source": [
        "ls 'drive/MyDrive/file2/vdo/Isaree/01_karnmay.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUZ5VZmZbFWi"
      },
      "source": [
        "ls 'drive/MyDrive/file2/done/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFotJmV_esfP"
      },
      "source": [
        "# ls 'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/model_FERG_DB_256'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxch2_rrRXwL"
      },
      "source": [
        "pred_emotion= []\n",
        "\n",
        "filepath = 'drive/MyDrive/file2/done/face2_01_KARNMAY_P1.csv'\n",
        "vdopath ='drive/MyDrive/file2/vdo/Isaree/01_karnmay.mp4'\n",
        "df = pd.read_csv(filepath)\n",
        "dim = (224, 224)\n",
        "rowimg = []\n",
        "label_img = []\n",
        "print('df', df.shape)\n",
        "\n",
        "for i in tqdm(range(df.shape[0])):\n",
        "  seqimg = []\n",
        "  for j in range(df['start_frame'][i], df['end_frame'][i], 3):\n",
        "    cap = cv2.VideoCapture(vdopath)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, j)\n",
        "    success, image = cap.read()\n",
        "    faceimg = detector.detect_faces(image)\n",
        "    try:\n",
        "      pos = faceimg[0][:4]\n",
        "      cropimg =image[int(pos[1]):int(pos[3]),int(pos[0]):int(pos[2])] \n",
        "      resized = cv2.resize(cropimg, dim, interpolation=cv2.INTER_CUBIC)     \n",
        "      image = preprocess_input(resized)\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      preds = model_4.predict(image)\n",
        "      pred_class = np.argmax(preds)\n",
        "      seqimg.append(pred_class) \n",
        "    except:\n",
        "      print(\"Emotion Error  i = \",i, ' j =', j)\n",
        "\n",
        "  seq_label = my_mode(seqimg)\n",
        "  print('i =', i, 'seq_label =', seq_label)\n",
        "  try:\n",
        "    pred_emotion.append(seq_label[0])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3647d4e5ef794145897ed45d83204389",
            "235cbea959fb4ef89b042dfcd77bc8f8",
            "073b46307b9e4c5a97292293ca0828ce",
            "0b609aad7d4d469892a0022fe25caf06",
            "9b009c708c514eb5850cf2fcac30f365",
            "ed11d1b10a4d4a87920db69847425930",
            "917ccd063364453ebe7a0b5c8bdce712",
            "e271a4bbce1c4038ac0b10a259df3372"
          ]
        },
        "id": "FVO_hl0v8WK-",
        "outputId": "ca35f11c-113d-42d7-d253-4ffdc6c0299f"
      },
      "source": [
        "pred_emotion= []\n",
        "\n",
        "filepath = 'drive/MyDrive/file2/done/face2_01_KARNMAY_P1.csv'\n",
        "vdopath ='drive/MyDrive/file2/vdo/Isaree/01_karnmay.mp4'\n",
        "df = pd.read_csv(filepath)\n",
        "dim = (224, 224)\n",
        "rowimg = []\n",
        "label_img = []\n",
        "print('df', df.shape)\n",
        "\n",
        "for i in tqdm(range(86, df.shape[0])):\n",
        "  seqimg = []\n",
        "  for j in range(df['start_frame'][i], df['end_frame'][i], 3):\n",
        "    cap = cv2.VideoCapture(vdopath)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, j)\n",
        "    success, image = cap.read()\n",
        "    faceimg = detector.detect_faces(image)\n",
        "    try:\n",
        "      pos = faceimg[0][:4]\n",
        "      cropimg =image[int(pos[1]):int(pos[3]),int(pos[0]):int(pos[2])] \n",
        "      resized = cv2.resize(cropimg, dim, interpolation=cv2.INTER_CUBIC)     \n",
        "      image = preprocess_input(resized)\n",
        "      image = np.expand_dims(image, axis=0)\n",
        "      preds = model_4.predict(image)\n",
        "      pred_class = np.argmax(preds)\n",
        "      seqimg.append(pred_class) \n",
        "    except:\n",
        "      print(\"Emotion Error  i = \",i, ' j =', j)\n",
        "\n",
        "  seq_label = my_mode(seqimg)\n",
        "  print('i =', i, 'seq_label =', seq_label)\n",
        "  try:\n",
        "    pred_emotion.append(seq_label[0])\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df (152, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3647d4e5ef794145897ed45d83204389",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=66.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "i = 86 seq_label = [2]\n",
            "i = 87 seq_label = [2]\n",
            "i = 88 seq_label = [2]\n",
            "i = 89 seq_label = [2]\n",
            "i = 90 seq_label = [2]\n",
            "i = 91 seq_label = [2]\n",
            "i = 92 seq_label = [2]\n",
            "i = 93 seq_label = [2]\n",
            "i = 94 seq_label = [2]\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51860\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51863\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51866\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51869\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51872\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51875\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51878\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51881\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51884\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51887\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51890\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51893\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51896\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51899\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51902\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51905\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51908\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51911\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51914\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51917\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51920\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51923\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51926\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51929\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51932\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51935\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51938\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51941\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51944\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51947\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51950\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51953\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51956\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51959\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51962\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51965\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51968\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51971\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51974\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51977\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51980\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51983\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51986\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51989\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51992\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51995\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 51998\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52001\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52004\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52007\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52010\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52013\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52016\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52019\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52022\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52025\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52028\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52031\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52034\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52037\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52040\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52043\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52046\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52049\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52052\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52055\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52058\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52061\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52064\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52067\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52070\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52073\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52076\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  95  j = 52079\n",
            "i = 95 seq_label = [2]\n",
            "i = 96 seq_label = [2]\n",
            "i = 97 seq_label = [2]\n",
            "i = 98 seq_label = [2]\n",
            "i = 99 seq_label = [2]\n",
            "i = 100 seq_label = [2]\n",
            "i = 101 seq_label = [2]\n",
            "i = 102 seq_label = [2]\n",
            "i = 103 seq_label = [2]\n",
            "i = 104 seq_label = [2]\n",
            "i = 105 seq_label = [2]\n",
            "i = 106 seq_label = [2]\n",
            "i = 107 seq_label = [2]\n",
            "i = 108 seq_label = [2]\n",
            "i = 109 seq_label = []\n",
            "i = 110 seq_label = [2]\n",
            "i = 111 seq_label = [2]\n",
            "i = 112 seq_label = [2]\n",
            "i = 113 seq_label = [2]\n",
            "i = 114 seq_label = [2]\n",
            "i = 115 seq_label = [2]\n",
            "i = 116 seq_label = [2]\n",
            "i = 117 seq_label = [1]\n",
            "i = 118 seq_label = [2]\n",
            "i = 119 seq_label = [2]\n",
            "i = 120 seq_label = [2]\n",
            "i = 121 seq_label = [2]\n",
            "i = 122 seq_label = [2]\n",
            "i = 123 seq_label = [2]\n",
            "i = 124 seq_label = [2]\n",
            "i = 125 seq_label = [2]\n",
            "i = 126 seq_label = [2]\n",
            "i = 127 seq_label = [2]\n",
            "i = 128 seq_label = [2]\n",
            "i = 129 seq_label = [2]\n",
            "i = 130 seq_label = [2]\n",
            "i = 131 seq_label = [2]\n",
            "i = 132 seq_label = [2]\n",
            "i = 133 seq_label = [2]\n",
            "i = 134 seq_label = [2]\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  135  j = 83376\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  135  j = 83379\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  135  j = 83382\n",
            "Warning: NO FACE is detected\n",
            "Emotion Error  i =  135  j = 83385\n",
            "i = 135 seq_label = [2]\n",
            "i = 136 seq_label = [2]\n",
            "i = 137 seq_label = [2]\n",
            "i = 138 seq_label = [2]\n",
            "i = 139 seq_label = [2]\n",
            "i = 140 seq_label = [2]\n",
            "i = 141 seq_label = [2]\n",
            "i = 142 seq_label = [2]\n",
            "i = 143 seq_label = [2]\n",
            "i = 144 seq_label = [2]\n",
            "i = 145 seq_label = [2]\n",
            "i = 146 seq_label = [2]\n",
            "i = 147 seq_label = [2]\n",
            "i = 148 seq_label = [2]\n",
            "i = 149 seq_label = [2]\n",
            "i = 150 seq_label = [2]\n",
            "i = 151 seq_label = [2]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t6auxbvnSsN",
        "outputId": "998fb580-70f9-49d8-a88b-d9a31a2915a7"
      },
      "source": [
        "len(pred_emotion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g2Mo0Ry_nM-0",
        "outputId": "579ceaf2-b9ab-430e-f73b-1bf40aed5084"
      },
      "source": [
        "df = pd.DataFrame(np.array(pred_emotion), columns=['label'])\n",
        "from google.colab import files\n",
        "df.to_csv('face2_01_KARNMAY_model_ck_kaggle_jaffe.csv') \n",
        "files.download('face2_01_KARNMAY_model_ck_kaggle_jaffe.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ef1d58fe-3d76-4b9c-afc6-41e0520a0017\", \"face_7_2_01_thematter_FERG_DB_256.csv\", 47)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1TQ1XKAUr_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "667fe58a-fa4d-433f-e317-38303bc8d71c"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-534b7a74019f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGVa4i0rPLgf"
      },
      "source": [
        "dic_resmask =  {0:'anger',1:'disgust', 2:'fear', 3:'happiness',4:'sadness',\t5:'surprise',\t6:'neutral' }\n",
        "# dic_kagle =  {0:'anger',1:'disgust', 2:'fear', 3:'happiness',4:'sadness',\t5:'surprise',\t6:'neutral' }\n",
        "# dic_ckplus =  {1:'anger',3:'disgust', 4:'fear', 5:'happiness',6:'sadness',\t7:'surprise'}\n",
        "# dic_fer_svm_rf =  {0:'anger',1:'disgust', 2:'fear', 3:'happiness',4:'sadness',\t5:'surprise',\t6:'neutral' }\n",
        "# dic_fer_svm_rf =  {0:'anger',1:'disgust', 2:'fear', 3:'happiness',4:'sadness',\t5:'surprise',\t6:'neutral' }\n",
        "# dic_Deepfake =  {0:'anger',1:'fear', 2:'neutral', 3:'sad',4:'disgust',\t5:'happy',\t6:'surprise' }\n",
        "\n",
        "pred = pd.DataFrame({'Pred':pred_emotion})\n",
        "pred['emotion'] = pred['Pred'].replace(dic_resmask) \n",
        "result = pd.concat([df, pred], axis=1, join=\"inner\")\n",
        "\n",
        "from google.colab import files\n",
        "result.to_csv('05_pimrypie_tony_R1.csv') \n",
        "files.download('05_pimrypie_tony_R1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}