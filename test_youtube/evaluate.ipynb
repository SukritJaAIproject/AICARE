{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colabpro_testresult.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99f4f07f6dbf496fb25cb5f84765991d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7eb47b1151924a718b8494d8ed5526a7",
              "IPY_MODEL_7b7a5818b5194f38b63019c5825432e6"
            ],
            "layout": "IPY_MODEL_aa927b00dfb24079a03a7e3135239053"
          }
        },
        "7eb47b1151924a718b8494d8ed5526a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": " 49%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71b2aaa328be4278a9149d576b7e7235",
            "max": 136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6365a58ca19f443fa2a9a6ca3c1afed7",
            "value": 66
          }
        },
        "7b7a5818b5194f38b63019c5825432e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cf938649154724a4d06f7c3f9b6580",
            "placeholder": "​",
            "style": "IPY_MODEL_76322ef1de2e47c3b42b6fe43592c37a",
            "value": " 66/136 [4:55:50&lt;2:20:45, 120.65s/it]"
          }
        },
        "aa927b00dfb24079a03a7e3135239053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71b2aaa328be4278a9149d576b7e7235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6365a58ca19f443fa2a9a6ca3c1afed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "98cf938649154724a4d06f7c3f9b6580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76322ef1de2e47c3b42b6fe43592c37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPFVCHjcQrEv",
        "outputId": "87cc59ce-5765-4cd3-9818-7204a9969d56"
      },
      "source": [
        "!pip install -q py-feat\n",
        "# !pip install -q facial-emotion-recognition\n",
        "# !pip install -q fer\n",
        "# !pip install -q deepface\n",
        "!pip install -q mtcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.3MB 7.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpu_xoUpRHxW",
        "outputId": "75774ac9-3e56-495e-c85c-8233870c5183"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from feat import Detector\n",
        "import glob\n",
        "import math\n",
        "import dlib\n",
        "import joblib\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import Counter\n",
        "from mtcnn import MTCNN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2v82PSkQ0Fu",
        "outputId": "89c7f325-ffc8-45f5-a8a1-676dc10cde64"
      },
      "source": [
        "################ FER\n",
        "\n",
        "# https://github.com/justinshenk/fer\n",
        "# from fer import FER\n",
        "# import operator\n",
        "# # detector = FER()\n",
        "# detector = FER(mtcnn=True)\n",
        "\n",
        "################ DeepFace\n",
        "# from deepface import DeepFace\n",
        "# https://awesomeopensource.com/project/serengil/deepface\n",
        "# obj = DeepFace.analyze(img_path = \"img4.jpg\", actions = ['age', 'gender', 'race', 'emotion'])\n",
        "# print(obj[\"age\"],\" years old \",obj[\"dominant_race\"],\" \",obj[\"dominant_emotion\"],\" \", obj[\"gender\"])\n",
        "\n",
        "################# py-feat\n",
        "\n",
        "# from feat import Detector\n",
        "face_model = \"MTCNN\"\n",
        "landmark_model = \"PFLD\"\n",
        "au_model = \"rf\"\n",
        "emotion_model = \"resmasknet\" #resmasknet,fer, svm, rf\n",
        "detector = Detector(face_model = face_model, landmark_model = landmark_model, au_model = au_model, emotion_model = emotion_model)\n",
        "\n",
        "detector_MTCNN = MTCNN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Face Detection model:  MTCNN\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/onet.npy\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/pnet.npy\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/rnet.npy\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/pfld_model_best.pth.tar\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/hog_pca_all_emotio.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/hog_pca_all_emotio.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/hog_scalar_aus.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/RF_568.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/hog_pca_all_emotio.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/hog_scalar_aus.joblib\n",
            "Using downloaded and verified file: /usr/local/lib/python3.7/dist-packages/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
            "Loading Face Landmark model:  PFLD\n",
            "Loading au model:  rf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading emotion model:  resmasknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWwugvO9mv9k"
      },
      "source": [
        "def cal_mean_std(image):\n",
        "  val = np.reshape(image[:,:,0], -1)\n",
        "  img_mean = np.mean(val)\n",
        "  img_std = np.std(val)\n",
        "  X = np.array([img_mean, img_std]).reshape(1, -1)\n",
        "  return X\n",
        "\n",
        "def my_mode(sample):\n",
        "  c = Counter(sample)\n",
        "  return [k for k, v in c.items() if v == c.most_common(1)[0][1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsrISV-6mwO"
      },
      "source": [
        "### List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSDYTv2W-Kh2"
      },
      "source": [
        "# datalist = ['Anongrat_06_810', 'Anongrat_06_811', 'Anongrat_06_812', 'Anongrat_06_813', 'Anongrat_06_814',\n",
        "#             'Anongrat_06_815', 'Anongrat_06_816', 'Anongrat_06_817', 'Anongrat_06_818', 'Anongrat_06_819',\n",
        "#             'Anongrat_06_8110', 'Anongrat_06_8111', 'Anongrat_06_8112', 'Anongrat_06_8113', 'Anongrat_06_8114',\n",
        "#             'Anongrat_06_8115', 'Anongrat_06_8116', 'Anongrat_06_8117', 'Anongrat_06_8118', 'Anongrat_06_8119',\n",
        "#             'Anongrat_06_8120', 'Anongrat_06_8121']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM9KyUKU6pdZ"
      },
      "source": [
        "predf_2 = []\n",
        "actf_2 = []\n",
        "for name in datalist:\n",
        "  data = np.load('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/label_emotion/prepare_data/new/Anongrat/'+ name +'.npy', allow_pickle=True)\n",
        "  for i in tqdm(range(data.shape[0])):\n",
        "    seqimg   =  []\n",
        "    for j in range(len(data[i][0])):\n",
        "      img = data[i][0][j]\n",
        "      label = data[i][1]\n",
        "      # cv2_imshow(img)\n",
        "      try:\n",
        "        detected_faces = [[0, 0, img.shape[0]-1, img.shape[1]-1, 1]]\n",
        "        detected_landmarks = detector.detect_landmarks(img, detected_faces)\n",
        "        ypred = detector.detect_emotions(img, detected_faces, detected_landmarks)[0]\n",
        "        ypred_arg = np.argmax(ypred)\n",
        "      except:\n",
        "        print(\"error i = \",i, ' j =', j)\n",
        "        pass\n",
        "      seqimg.append(ypred_arg) \n",
        "    seq_label = my_mode(seqimg)\n",
        "    print('seq_label', seq_label)\n",
        "    predf_2.append(seq_label[0])\n",
        "    actf_2.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7krkkGmy6sf_"
      },
      "source": [
        "# #svm, rf\n",
        "# image_prediction = detector.detect_image(img)\n",
        "# ypred = image_prediction.emotions()\n",
        "# ypred_arg = np.argmax(ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBNhHGjwBwof"
      },
      "source": [
        "## Single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vLvEOlrGHN"
      },
      "source": [
        "predf_2 = []\n",
        "actf_2 = []\n",
        "data = np.load('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/label_emotion/prepare_data/new/Anongrat/Anongrat_06_8116.npy', allow_pickle=True)\n",
        "for i in tqdm(range(data.shape[0])):\n",
        "  seqimg   =  []\n",
        "  for j in range(len(data[i][0])):\n",
        "    img = data[i][0][j]\n",
        "    label = data[i][1]\n",
        "    try:\n",
        "      detected_faces = [[0, 0, img.shape[0]-1, img.shape[1]-1, 1]]\n",
        "      detected_landmarks = detector.detect_landmarks(img, detected_faces)\n",
        "      ypred = detector.detect_emotions(img, detected_faces, detected_landmarks)[0]\n",
        "      ypred_arg = np.argmax(ypred)\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    seqimg.append(ypred_arg) #ypred_arg\n",
        "  seq_label = my_mode(seqimg)\n",
        "  print('seq_label', seq_label)\n",
        "  predf_2.append(seq_label[0])\n",
        "  actf_2.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJRWrtxtL81X"
      },
      "source": [
        "# #svm, rf\n",
        "# image_prediction = detector.detect_image(img)\n",
        "# ypred = image_prediction.emotions()\n",
        "# ypred_arg = np.argmax(ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_abXwsw0B6ST"
      },
      "source": [
        "## Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9OKdqEw1rD"
      },
      "source": [
        "predf = []\n",
        "actf = []\n",
        "for files in glob.iglob(r'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/label_emotion/prepare_data/*.npy'):\n",
        "  print(files)\n",
        "  data = np.load(files, allow_pickle=True)\n",
        "  for i in tqdm(range(data.shape[0])):\n",
        "    seqimg   =  []\n",
        "    for j in range(len(data[i][0])):\n",
        "      img = data[i][0][j]\n",
        "      # cv2_imshow(img)\n",
        "      label = data[i][1]\n",
        "      try:\n",
        "        obj = DeepFace.analyze(img_path = img, actions = ['emotion'])\n",
        "        deepf = obj[\"dominant_emotion\"]\n",
        "      except:\n",
        "        print(\"error i =\", i, ' j =', j)\n",
        "        deepf = 'neutral'\n",
        "\n",
        "      if deepf == 'angry':\n",
        "        ypred_arg = 0\n",
        "      elif deepf == 'disgust':\n",
        "        ypred_arg = 1\n",
        "      elif deepf == 'fear':\n",
        "        ypred_arg = 2\n",
        "      elif deepf == 'happy':\n",
        "        ypred_arg = 3\n",
        "      elif deepf == 'sad':\n",
        "        ypred_arg = 4\n",
        "      elif deepf == 'surprise':\n",
        "        ypred_arg = 5\n",
        "      elif deepf == 'neutral':\n",
        "        ypred_arg = 6\n",
        "      \n",
        "      seqimg.append(ypred_arg) #ypred_arg\n",
        "    seq_label = my_mode(seqimg)\n",
        "    print('seq_label', seq_label)\n",
        "    predf.append(seq_label[0])\n",
        "    actf.append(label)\n",
        "  np.save('df_pred'+str(i)+'.npy', np.array(predf))\n",
        "  np.save('df_act'+str(i)+'.npy', np.array(actf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUrNBJdNRUJ8"
      },
      "source": [
        "## ALL STEP IN ONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99f4f07f6dbf496fb25cb5f84765991d",
            "7eb47b1151924a718b8494d8ed5526a7",
            "7b7a5818b5194f38b63019c5825432e6",
            "aa927b00dfb24079a03a7e3135239053",
            "71b2aaa328be4278a9149d576b7e7235",
            "6365a58ca19f443fa2a9a6ca3c1afed7",
            "98cf938649154724a4d06f7c3f9b6580",
            "76322ef1de2e47c3b42b6fe43592c37a"
          ]
        },
        "id": "Qxch2_rrRXwL",
        "outputId": "eb8046dd-ff8a-45af-8589-17c114ff0b8a"
      },
      "source": [
        "predf_2 = []\n",
        "actf_2 = []\n",
        "\n",
        "namefile = 'Wachiraya1.csv'\n",
        "vdo_link = 'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/label_emotion/emotional_labeling/Wachiraya/02_016.mp4'\n",
        "df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/Test/'+namefile)\n",
        "rowimg = []\n",
        "label_img = []\n",
        "df = df.loc[df['source'] == '02_016']\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print('df', df.shape)\n",
        "\n",
        "for i in tqdm(range(66,df.shape[0])):\n",
        "  seqimg = []\n",
        "  for j in range(df['start_frame'][i], df['end_frame'][i]):\n",
        "    cap = cv2.VideoCapture(vdo_link)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, j)\n",
        "    success, image = cap.read()\n",
        "    scale_percent = 50 \n",
        "    width = int(image.shape[1] * scale_percent / 100)\n",
        "    height = int(image.shape[0] * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    resized = cv2.resize(image, dim)\n",
        "    try:\n",
        "      faceimg = detector_MTCNN.detect_faces(resized)\n",
        "      faceimg = faceimg[0]['box']\n",
        "      faceimg =resized[faceimg[1]-30:faceimg[1]+faceimg[3]+30,faceimg[0]-30:faceimg[0]+faceimg[2]+30] #[x, y, width, height]  \n",
        "      # cv2_imshow(faceimg)\n",
        "    except:\n",
        "      try:\n",
        "        faceimg = detect_faces(resized)\n",
        "      except:\n",
        "        pass\n",
        "        # print(\"error i = \",i, ' j =', j)\n",
        "    try:\n",
        "      detected_faces = [[0, 0, faceimg.shape[0]-1, faceimg.shape[1]-1, 1]]\n",
        "      detected_landmarks = detector.detect_landmarks(faceimg, detected_faces)\n",
        "      ypred = detector.detect_emotions(faceimg, detected_faces, detected_landmarks)[0]\n",
        "      ypred_arg = np.argmax(ypred)\n",
        "      seqimg.append(ypred_arg) #ypred_arg\n",
        "    except:\n",
        "      pass\n",
        "  seq_label = my_mode(seqimg)\n",
        "  print('i =', i, 'seq_label =', seq_label)\n",
        "  predf_2.append(seq_label[0])\n",
        "  actf_2.append(label)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df (136, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99f4f07f6dbf496fb25cb5f84765991d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=136.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "i = 0 seq_label = [3]\n",
            "i = 1 seq_label = [6]\n",
            "i = 2 seq_label = [3]\n",
            "i = 3 seq_label = [6]\n",
            "i = 4 seq_label = [3]\n",
            "i = 5 seq_label = [6]\n",
            "i = 6 seq_label = [6]\n",
            "i = 7 seq_label = [6]\n",
            "i = 8 seq_label = [3]\n",
            "i = 9 seq_label = [6]\n",
            "i = 10 seq_label = [6]\n",
            "i = 11 seq_label = [6]\n",
            "i = 12 seq_label = [6]\n",
            "i = 13 seq_label = [3]\n",
            "i = 14 seq_label = [6]\n",
            "i = 15 seq_label = [6]\n",
            "i = 16 seq_label = [6]\n",
            "i = 17 seq_label = [3]\n",
            "i = 18 seq_label = [6]\n",
            "i = 19 seq_label = [6]\n",
            "i = 20 seq_label = [6]\n",
            "i = 21 seq_label = [6]\n",
            "i = 22 seq_label = [3]\n",
            "i = 23 seq_label = [3]\n",
            "i = 24 seq_label = [6]\n",
            "i = 25 seq_label = [6]\n",
            "i = 26 seq_label = [6]\n",
            "i = 27 seq_label = [6]\n",
            "i = 28 seq_label = [6]\n",
            "i = 29 seq_label = [6]\n",
            "i = 30 seq_label = [6]\n",
            "i = 31 seq_label = [6]\n",
            "i = 32 seq_label = [6]\n",
            "i = 33 seq_label = [6]\n",
            "i = 34 seq_label = [6]\n",
            "i = 35 seq_label = [6]\n",
            "i = 36 seq_label = [6]\n",
            "i = 37 seq_label = [6]\n",
            "i = 38 seq_label = [6]\n",
            "i = 39 seq_label = [6]\n",
            "i = 40 seq_label = [6]\n",
            "i = 41 seq_label = [3]\n",
            "i = 42 seq_label = [6]\n",
            "i = 43 seq_label = [6]\n",
            "i = 44 seq_label = [3, 6]\n",
            "i = 45 seq_label = [6]\n",
            "i = 46 seq_label = [6]\n",
            "i = 47 seq_label = [6]\n",
            "i = 48 seq_label = [6]\n",
            "i = 49 seq_label = [3]\n",
            "i = 50 seq_label = [6]\n",
            "i = 51 seq_label = [6]\n",
            "i = 52 seq_label = [6]\n",
            "i = 53 seq_label = [6]\n",
            "i = 54 seq_label = [6]\n",
            "i = 55 seq_label = [6]\n",
            "i = 56 seq_label = [6]\n",
            "i = 57 seq_label = [3]\n",
            "i = 58 seq_label = [6]\n",
            "i = 59 seq_label = [6]\n",
            "i = 60 seq_label = [3]\n",
            "i = 61 seq_label = [3]\n",
            "i = 62 seq_label = [6]\n",
            "i = 63 seq_label = [6]\n",
            "i = 64 seq_label = [3]\n",
            "i = 65 seq_label = [6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2rfms_wFyUf",
        "outputId": "8a586cd7-bfc3-4d17-9d1b-af9df5013bba"
      },
      "source": [
        "len(predf_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1TQ1XKAUr_N"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vzOIeBOdjpxb"
      },
      "source": [
        "predf = []\n",
        "actf = []\n",
        "for files in glob.iglob(r'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/label_emotion/prepare_data/*.npy'):\n",
        "  print(files)\n",
        "  data = np.load(files, allow_pickle=True)\n",
        "  for i in tqdm(range(data.shape[0])):\n",
        "    seqimg   =  []\n",
        "    for j in range(len(data[i][0])):\n",
        "      img = data[i][0][j]\n",
        "      label = data[i][1]\n",
        "\n",
        "      ###################### mean std ###########################\n",
        "      # img = img.astype('float32')\n",
        "      # image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.0\n",
        "      # X = cal_mean_std(image)\n",
        "      # ypred = model.predict(X)[0]\n",
        "          \n",
        "      ###################### mean std landmark ###########################\n",
        "      # image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.0\n",
        "      # X = cal_mean_std(image).tolist()[0]\n",
        "      # try:\n",
        "      #   data_fea = np.array(get_landmarks(img)['landmarks_vectorised']).tolist()\n",
        "      #   x_test = np.array(X+data_fea).reshape(1,-1)\n",
        "      #   ypred = model.predict(x_test)[0]\n",
        "      # except:\n",
        "      #   ypred = 6\n",
        "    \n",
        "      ###################### Py-feat resmasknet, Fer ###########################\n",
        "      detected_faces = [[0, 0, img.shape[0]-1, img.shape[1]-1, 1]]\n",
        "      detected_landmarks = detector.detect_landmarks(img, detected_faces)\n",
        "      ypred = detector.detect_emotions(img, detected_faces, detected_landmarks)[0]\n",
        "      ypred_arg = np.argmax(ypred)\n",
        "\n",
        "      ###################### Py-feat svm, rf ###################################\n",
        "      # image_prediction = detector.detect_image(img)\n",
        "      # ypred = image_prediction.emotions()\n",
        "      # ypred_arg = np.argmax(ypred)\n",
        "\n",
        "      # emotion, score = detector.top_emotion(img)\n",
        "      # emotion = detector.detect_emotions(img)[0]['emotions']\n",
        "      # ypred_arg = max(emotion.items(), key=operator.itemgetter(1))[0]\n",
        "\n",
        "      # if emotion == 'anger':\n",
        "      #   ypred_arg = 0\n",
        "      # elif emotion == 'disgust':\n",
        "      #   ypred_arg = 1\n",
        "      # elif emotion == 'fear':\n",
        "      #   ypred_arg = 2\n",
        "      # elif emotion == 'happy':\n",
        "      #   ypred_arg = 3\n",
        "      # elif emotion == 'sad':\n",
        "      #   ypred_arg = 4\n",
        "      # elif emotion == 'surprise':\n",
        "      #   ypred_arg = 5\n",
        "      # elif emotion == 'neutral':\n",
        "      #   ypred_arg = 6\n",
        "\n",
        "      # obj = DeepFace.analyze(img_path = img, actions = ['age','emotion'])\n",
        "      # print(obj[\"dominant_emotion\"])\n",
        "      # deepf = obj[\"dominant_emotion\"]\n",
        "\n",
        "      # if deepf == 'angry':\n",
        "      #   ypred_arg = 0\n",
        "      # elif deepf == 'disgust':\n",
        "      #   ypred_arg = 1\n",
        "      # elif deepf == 'fear':\n",
        "      #   ypred_arg = 2\n",
        "      # elif deepf == 'happy':\n",
        "      #   ypred_arg = 3\n",
        "      # elif deepf == 'sad':\n",
        "      #   ypred_arg = 4\n",
        "      # elif deepf == 'surprise':\n",
        "      #   ypred_arg = 5\n",
        "      # elif deepf == 'neutral':\n",
        "      #   ypred_arg = 6\n",
        "      \n",
        "      seqimg.append(ypred_arg) #ypred_arg\n",
        "    seq_label = my_mode(seqimg)\n",
        "    print('seq_label', seq_label)\n",
        "    predf.append(seq_label[0])\n",
        "    actf.append(label)\n",
        "  np.save('df_pred'+str(i)+'.npy', np.array(predf))\n",
        "  np.save('df_act'+str(i)+'.npy', np.array(actf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkk5rSEGqjjs"
      },
      "source": [
        "# label_kaggle = (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
        "# label_ck+ = (1=anger\", \"3=disgust\", \"4=fear\", \"5=happy\", \"6=sadness\", \"7=surprise\") \n",
        "# resmasknet = ( anger=0,\tdisgust=1, fear=2, happiness=3,\tsadness=4,\tsurprise=5,\tneutral=6 )\n",
        "# fer, svm, rf = ( anger=0,\tdisgust=1, fear=2, happiness=3,\tsadness=4,\tsurprise=5,\tneutral=6 )\n",
        "# fer             = ( anger=0, 'disgust=1,fear=2,  happy=3,      sad=4,        surprise=5, neutral=6 )\n",
        "# Deepfake    = (angry=0, fear=1, neutral=2, sad=3, disgust=4, happy=5 and surprise = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPxM7bn5sDYe"
      },
      "source": [
        "result = pd.DataFrame({'label':actf_2, 'pred' : predf_2}).replace({'happy':3, 'surprise':5, 'relax':6, 'Neutral':6, 'neutral':6,'stress':0,'anger':0,'sad':4  }) # fer 1\n",
        "# result = result[result.pred != 2]\n",
        "result = result[result.pred != 2]\n",
        "result = result[result.pred != 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHZautpfsf7F",
        "outputId": "c57516ef-1130-4507-f64a-a18492302642"
      },
      "source": [
        "print(pd.unique(result['pred']))\n",
        "print(pd.unique(result['label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3]\n",
            "[0 6 4 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHvhLe6ogsTX"
      },
      "source": [
        "df = pd.read_csv('/content/result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8r9dXUso-yA"
      },
      "source": [
        "# ground_truth = pd.DataFrame(data[:,1], columns=['lable']).replace({'happy':5, 'surprise':7, 'relax':0, 'Neutral':0}) # ck+\n",
        "# ground_truth = pd.DataFrame(data[:,1], columns=['lable']).replace({'happy':3, 'surprise':5, 'relax':6, 'Neutral':6}) # kaggle\n",
        "# ground_truth = pd.DataFrame(data[:,1], columns=['lable']).replace({'happy':3, 'surprise':5, 'relax':6, 'Neutral':6}) # resmasknet\n",
        "result = pd.concat([result['label'], result['pred']], axis=1, join=\"inner\")\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "uPMoB-qOsEpJ",
        "outputId": "c70614f9-0387-4850-d4c6-5ee3eaff4c29"
      },
      "source": [
        "from google.colab import files\n",
        "result.to_csv('result_6.csv') \n",
        "files.download('result_6.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f04c282c-fa14-4961-9114-f442923c6c14\", \"result_6.csv\", 156)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "4lhqjeIqtLUN",
        "outputId": "fcf17b93-ecca-45d2-9142-7571da81e861"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "def plot_confus(pred_lable, truelable):\n",
        "  print(metrics.classification_report(pred_lable, truelable))\n",
        "  print('Accuracy: ',metrics.accuracy_score(truelable, pred_lable))\n",
        "\n",
        "  mat = confusion_matrix(truelable, pred_lable)\n",
        "  sns.heatmap(mat.T, square=True, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
        "  # tick_marks = np.arange(len(target_names))\n",
        "  # plt.xticks(tick_marks, target_names, rotation ='vertical', ha='center')\n",
        "  # plt.yticks(tick_marks, target_names, rotation ='horizontal', ha='center')\n",
        "  plt.xlabel('true label')\n",
        "  plt.ylabel('predicted label'); \n",
        "\n",
        "plot_confus(df['pred'], df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           3       0.94      0.21      0.34        76\n",
            "           4       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         0\n",
            "           6       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.21        77\n",
            "   macro avg       0.19      0.04      0.07        77\n",
            "weighted avg       0.93      0.21      0.34        77\n",
            "\n",
            "Accuracy:  0.2077922077922078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJaWXyjfQZbQ"
      },
      "source": [
        "# https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_cm(y_true, y_pred, figsize=(10,10)):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
        "    \n",
        "plot_cm(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugaIH8Axz3J1"
      },
      "source": [
        "# https://medium.com/clique-org/how-to-create-a-face-recognition-model-using-facenet-keras-fd65c0b092f1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}