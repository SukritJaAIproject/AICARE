{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Aicare_30052564_rev2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8167903362284e86aebf703c9fc869f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fd2c769e86d40f3a3ff274b7cf3a4db",
              "IPY_MODEL_2bb364f0f8894563877a7e43f01833e4"
            ],
            "layout": "IPY_MODEL_c367b119b1d449af9dfb3cfd2b7892cd"
          }
        },
        "3fd2c769e86d40f3a3ff274b7cf3a4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04a7934eac9c4ae493c73f9b1b99a1e9",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61de855b39274e639ba576571d544198",
            "value": 212
          }
        },
        "2bb364f0f8894563877a7e43f01833e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6d7a8e4d0554c208131e7ce71f94c68",
            "placeholder": "​",
            "style": "IPY_MODEL_bcf9e46b030b469196c0a70b63a0c0cd",
            "value": " 212/212 [42:14&lt;00:00, 11.96s/it]"
          }
        },
        "c367b119b1d449af9dfb3cfd2b7892cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04a7934eac9c4ae493c73f9b1b99a1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61de855b39274e639ba576571d544198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "d6d7a8e4d0554c208131e7ce71f94c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf9e46b030b469196c0a70b63a0c0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9HLdIFR-tqj"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hORFtMP9D7Od"
      },
      "source": [
        "**Available model**\n",
        "1. rf_more_6class : `(std + mean), ck+`\n",
        "2. knn_more : `(std + mean), ck+`\n",
        "3. rf_fewer : `(std + mean), ck+`\n",
        "4. knn_fewer : `(std + mean), ck+`\n",
        "5. resmasknet (Py-feat) : - `Facial expression recognition using residual masking network by (Pham et al., 2020)`\n",
        "6. fernet (Py-feat) - `Deep convolutional network`\n",
        "7. svm (Py-feat) - `SVM model trained on Histogram of Oriented Gradients extracted from ExpW, CK+, and JAFFE datasets`\n",
        "8. clf_lin_svm : `Pos landmark, Dis from center, Angle rel to center`\n",
        "9. clf_poly_svm : `Pos landmark, Dis from center, Angle rel to center`\n",
        "10. vgg16 : \n",
        "`- vgg16 bottleneck features`\n",
        "11. xception : - `xception bottleneck features`\n",
        "12. aug_vgg : `augmentation image`\n",
        "13. proposed model: \n",
        "* `( std + mean + landmark + Dist + Angle )` \n",
        "* `train from ck, kaggle`\n",
        "14. ANN model\n",
        "\n",
        "---\n",
        "**Available data**\n",
        "1. ck, ck+ : `https://drive.google.com/drive/folders/1pmmZfIwnuSajCafojd_WsaJ70LMutHmM?usp=sharing`\n",
        "2. kaggle (FER2013) `https://drive.google.com/drive/folders/1pQAE95FB5oUWA6QqBne8j9THzl0n3A4p?usp=sharing`\n",
        "3. JAFFE (เป็น dataset ของคนญี่ปุ่น) `https://drive.google.com/drive/folders/1isqq_HY9LxLo829E7UgLfv1AExT9nKuG?usp=sharing`\n",
        "4. AffectNet : `(อาจารย์กลกรณ์กำลังขอให้)`\n",
        "5. Aff-Wild2 `(https://drive.google.com/drive/folders/1x0SpKg-4335DbPw2qus6G7E9sFqwiS69?usp=sharing)` : emotion + au\n",
        "6. AFEW-VA : `https://ibug.doc.ic.ac.uk/resources/afew-va-database/` \n",
        "ระดับความรุ่นแรงของอารมณ์\n",
        "7. FERG-DB : `https://drive.google.com/drive/folders/1M6z2nA0MULuP2Wl89EES9EVtK4Nx_L3z?usp=sharing`\n",
        "8. RAVDESS : `https://drive.google.com/drive/folders/1bxtpkLiozXwKIr6QzdBPHwOrhKO5_Jw3?usp=sharing`\n",
        "    * `https://www.youtube.com/watch?v=cxMK2J0P7J0`\n",
        "    * `https://zenodo.org/record/3255102#.YLYlv6gzaUk`\n",
        "\n",
        "9. `https://en.wikipedia.org/wiki/Facial_expression_databases#cite_note-11`\n",
        "\n",
        "---\n",
        "**Function**\n",
        "1. convert_class\n",
        "2. convert_pred\n",
        "3. load_dataset\n",
        "4. prepare_kaggle\n",
        "5. load_model\n",
        "6. cal_mean_std\n",
        "7. plot_confus\n",
        "8. resmasknet_pred\n",
        "9. evaluate_model\n",
        "---\n",
        "\n",
        "\n",
        "1.   label_ck+  = `(1=anger\", \"3=disgust\", \"4=fear\", \"5=happy\", \"6=sadness\", \"7=surprise\")`\n",
        "rf_more_6class\n",
        "2. label_kaggle  = ` (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).`\n",
        "3. JAFFE = `[5=happy, 6=sadness, 7=surprise, 1=anger, 3=disgust, 4=fear]`\n",
        "4. label_me      = [ \"0\",      \"1\",      \"2\",    \"3\",      \"4\",        \"5\"   ] \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-NBLSdqby_B",
        "outputId": "6274ddb8-7981-4de1-c272-fb5896043bbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B2twUVM6567"
      },
      "source": [
        "import pandas as pd\n",
        "import dlib\n",
        "import joblib\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import time\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from drive.MyDrive.AIHealthcare.AIcare_Phrase1.resmarknet_test import *\n",
        "from drive.MyDrive.AIHealthcare.AIcare_Phrase1.fernet_model import *\n",
        "from drive.MyDrive.AIHealthcare.AIcare_Phrase1.fernet_test import *\n",
        "from drive.MyDrive.AIHealthcare.AIcare_Phrase1.helper import *\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.xception import preprocess_input as xception_preprocess\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37hil3awHayc"
      },
      "source": [
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/kaggle/fer2013/fer2013.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ9TtCIzqckZ"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gfj1YSy8Y5V"
      },
      "source": [
        "## convert_class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3efHIc98cPb"
      },
      "source": [
        "def convert_class(df, data_name):\n",
        "  ##################### midframe \n",
        "  if data_name == 'kaggle':\n",
        "    df.drop(df[df.lable == 6].index, inplace=True)\n",
        "    df = df.reset_index(drop=True)\n",
        "  elif data_name == 'ck+':\n",
        "    df[\"lable\"].replace({1:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace=True)\n",
        "  elif data_name == 'jaffe':\n",
        "    df[\"lable\"].replace({1:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace=True)\n",
        "  print('convert_class', pd.unique(df['lable']))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1R2B9j_8Y7b"
      },
      "source": [
        "## convert_pred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6pwoaWu8fn6"
      },
      "source": [
        "def convert_pred(df, model_name):\n",
        "  if model_name in ['resmasknet', 'clf_lin_svm', 'clf_poly_svm']:\n",
        "    pass\n",
        "    # df[\"lable\"].replace({1:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace=True)\n",
        "  elif model_name in ['rf_more_6class', 'knn_more_6class', 'rf_fewer_6class',\n",
        "                      'knn_fewer_6class', 'rf_feature_6class', 'rf_ck_kaggle',\n",
        "                      'rf_ck_kaggle_jaffe']:\n",
        "    df[\"lable\"].replace({1:0, 3:1, 4:2, 5:3, 6:4, 7:5}, inplace=True)\n",
        "  print('convert_pred', df['lable'].unique())\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkGIShwZ8Y9-"
      },
      "source": [
        "## load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olVA-ArK7EY1",
        "outputId": "a54722df-9b00-4261-b6ee-e3b77c3d05b7"
      },
      "source": [
        "ls 'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_prepro_jaffe.csv  \u001b[0m\u001b[01;34mjaffedbase\u001b[0m/     JAFFE_df.csv     jaffe_int.csv\n",
            "jaffe.csv            jaffedbase.zip  jaffe_final.csv  README_FIRST.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks9uuM8S8jAD"
      },
      "source": [
        "def load_dataset(data_name):\n",
        "  if data_name == 'ck+':\n",
        "    midframe = np.load(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/mid_feame_6c.npy\", allow_pickle=True)\n",
        "    df = pd.DataFrame(midframe)\n",
        "    df.columns = ['image', 'lable']\n",
        "    df = convert_class(df, data_name)\n",
        "\n",
        "  elif data_name == 'kaggle':\n",
        "    df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/kaggle/fer2013/fer2013.csv')\n",
        "    X, Y = prepare_kaggle(df[df['Usage'] =='PrivateTest'])\n",
        "    df = pd.DataFrame({'image': X.tolist(),'lable': Y.tolist()})\n",
        "    df['image'] = df['image'].apply(lambda x: np.array(x,np.float32))\n",
        "    df = convert_class(df, data_name)\n",
        "  \n",
        "  elif data_name == 'jaffe':\n",
        "    df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/jaffe_final.csv')\n",
        "    # df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/df_prepro_jaffe.csv')\n",
        "    df = df[['path','lable']]\n",
        "    df.columns = ['image', 'lable']\n",
        "    df['image'] = df['image'].apply(lambda x: cv2.imread(x))\n",
        "    df[\"lable\"].replace({0:5, 1:6, 2:7, 3:1, 4:3, 5:4}, inplace=True)\n",
        "    df = convert_class(df, data_name)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E0Oye288Y_7"
      },
      "source": [
        "## prepare_kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shbBUb5l8nHE"
      },
      "source": [
        "def prepare_kaggle(data):\n",
        "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
        "    image_label = np.array(list(map(int, data['emotion'])))\n",
        "    for i, row in enumerate(data.index):\n",
        "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
        "        image = np.reshape(image, (48, 48))\n",
        "        image_array[i] = image\n",
        "    return image_array, image_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNFJDGVOyQsj"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPNOSv5Czwjr"
      },
      "source": [
        "# %cd drive/MyDrive/AIHealthcare/Emotion_Categ_from_Vdo_frame_img_SeqVoting/\n",
        "# emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"]\n",
        "# # emotions = [    \"0\",     \"1\",       \"2\",        \"3\",      \"4\",    \"5\",      \"6\",        \"7\"   ]\n",
        "# observations = glob.glob(\"source_emotion/*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nhha2IWS-7K"
      },
      "source": [
        "### --- CK+"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVRU-ItDyTR8"
      },
      "source": [
        "# neutral_file_dict={}\n",
        "# img_mean_list = []\n",
        "# img_std_list = []\n",
        "# lable_list = []\n",
        "# emo_list = []\n",
        "# landmarkls = []\n",
        "\n",
        "# for obs in tqdm(observations):  \n",
        "#     obs_id = str(obs[-4:]) # S028\n",
        "\n",
        "#     emotions_folders = \"{0}/*\".format(str(obs)) # source_emotion/S028/*\n",
        "#     emotions_sessions =  glob.glob(emotions_folders) # ['source_emotion/S028/001']\n",
        "    \n",
        "#     for each_emotion_session in emotions_sessions:\n",
        "#         emotion_sequence_folder = \"{0}/*\".format(each_emotion_session) #source_emotion/S028/001/*\n",
        "#         emotion_sequence_files = glob.glob(emotion_sequence_folder) #['source_emotion/S028/001/S028_001_00000024_emotion.txt']\n",
        "\n",
        "#         for emotion_output in emotion_sequence_files: #source_emotion/S028/001/S028_001_00000024_emotion.txt\n",
        "#             emotion_seq_no = emotion_output[20:23] #001\n",
        "#             file = open(emotion_output, 'r')\n",
        "#             emotion= int(float(file.readline())) #1\n",
        "#             peak_num_list = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))\n",
        "#             peak_num = len(peak_num_list)\n",
        "#             mid_num =  math.floor(peak_num/2.0)\n",
        "\n",
        "#             peak_frame = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[-1] #source_images/S028/001/S028_001_00000024.png\n",
        "#             mid_frame = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[mid_num] #source_images/S028/001/S028_001_00000024.png\n",
        "#             first_frame = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[0] #source_images/S028/001/S028_001_00000001.png\n",
        "\n",
        "#             for i in range(mid_num-1, peak_num):\n",
        "#               moreframe = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[i]\n",
        "#               image = cv2.imread(moreframe)\n",
        "#               face_image = detect_faces(image)\n",
        "#               image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)/255.0\n",
        "#               val = np.reshape(image[:,:,0], -1)\n",
        "#               img_mean = np.mean(val) # mean feature\n",
        "#               img_std = np.std(val) # std feature\n",
        "#               try :\n",
        "#                 data = np.array(get_landmarks(face_image)['landmarks_vectorised'])      \n",
        "#               except:\n",
        "#                 print('i', i) \n",
        "#               img_mean_list.append(img_mean)\n",
        "#               img_std_list.append(img_std)\n",
        "#               lable_list.append(emotion)\n",
        "#               landmarkls.append(data)\n",
        "\n",
        "# dict_more = {'meanf': img_mean_list, 'stdf': img_std_list, 'label': lable_list} \n",
        "# df_more_frame = pd.DataFrame(dict_more)\n",
        "\n",
        "# indexNames = df_more_frame[df_more_frame['label'] == 2].index\n",
        "# df_more_frame.drop(indexNames , inplace=True)\n",
        "# print('df_more_frame', df_more_frame.shape)\n",
        "# landmark_df = pd.DataFrame(np.array(landmarkls))\n",
        "# %cd ../../../../\n",
        "# df = pd.concat([df_more_frame, landmark_df], axis=1, join=\"inner\")\n",
        "# from google.colab import files\n",
        "# df.to_csv('df6c.csv') \n",
        "# files.download('df6c.csv')\n",
        "\n",
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/feature/df6c.csv')\n",
        "# df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG7FZOAXTGwy"
      },
      "source": [
        "### ---Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqTre9zjrgg3"
      },
      "source": [
        "# dfkaggle = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/kaggle/fer2013/fer2013.csv')\n",
        "# # dfkaggle.head(2)\n",
        "\n",
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/kaggle/fer2013/fer2013.csv')\n",
        "# X, Y = prepare_kaggle(df)\n",
        "# df = pd.DataFrame({'image': X.tolist(),'lable': Y.tolist()})\n",
        "# df['image'] = df['image'].apply(lambda x: np.array(x,np.float32))\n",
        "# # df.drop(df[df.lable == 6].index, inplace=True)\n",
        "# df = df.reset_index(drop=True)\n",
        "# df[\"lable\"].replace({0:1, 1:3, 2:4, 3:5, 4:6, 5:7}, inplace=True)\n",
        "\n",
        "# img_mean_list = []\n",
        "# img_std_list = []\n",
        "# lable_list = []\n",
        "# landmarkls = []\n",
        "\n",
        "# for i in tqdm(range(df.shape[0])):\n",
        "  \n",
        "#   gray = df['image'][i].astype('uint8')\n",
        "#   val = np.reshape(gray, -1)\n",
        "#   img_mean = np.mean(val) # mean feature\n",
        "#   img_std = np.std(val) # std feature\n",
        "#   # print('gray',gray.shape)\n",
        "#   try:\n",
        "#     data = np.array(get_landmarks(gray)['landmarks_vectorised'])\n",
        "#   except:\n",
        "#     # print('i = ',i)  \n",
        "#     img_mean = 0\n",
        "#     img_std = 0\n",
        "#   img_mean_list.append(img_mean)\n",
        "#   img_std_list.append(img_std)\n",
        "#   lable_list.append(df['lable'][i])\n",
        "#   landmarkls.append(data) \n",
        "#   np.append([[0, 1, 2], [3, 4, 5]],[[6, 7, 8]], axis=0)\n",
        "\n",
        "# print(len(img_mean_list))\n",
        "# print(len(img_std_list))\n",
        "# print(len(lable_list))\n",
        "# print(len(landmarkls))\n",
        "# df_kaggle = pd.DataFrame({'meanf': img_mean_list,'stdf': img_std_list, 'label':lable_list})\n",
        "# landmark_df = pd.DataFrame(np.array(landmarkls))\n",
        "# df_kaggle = pd.concat([df_kaggle, landmark_df], axis=1, join=\"inner\")\n",
        "# df_kaggle.drop(df_kaggle[df_kaggle.meanf == 0].index, inplace=True)\n",
        "\n",
        "# from google.colab import files\n",
        "# df_kaggle.to_csv('df_kaggle.csv') \n",
        "# files.download('df_kaggle.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xq0gR72JAo"
      },
      "source": [
        "### ---JAFFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFyUSKcu2NNo"
      },
      "source": [
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/jaffe_final.csv')\n",
        "# df = df[['path','lable']]\n",
        "# df.columns = ['image', 'lable']\n",
        "# df['image'] = df['image'].apply(lambda x: cv2.imread(x))\n",
        "# df[\"lable\"].replace({0:5, 1:6, 2:7, 3:1, 4:3, 5:4}, inplace=True)\n",
        "\n",
        "# img_mean_list = []\n",
        "# img_std_list = []\n",
        "# lable_list = []\n",
        "# landmarkls = []\n",
        "\n",
        "# for i in tqdm(range(df.shape[0])):\n",
        "#   gray = df['image'][i]\n",
        "#   val = np.reshape(gray, -1)\n",
        "#   img_mean = np.mean(val) # mean feature\n",
        "#   img_std = np.std(val) # std feature\n",
        "#   try:\n",
        "#     data = np.array(get_landmarks(gray)['landmarks_vectorised'])\n",
        "#   except:\n",
        "#     print('i = ',i)  \n",
        "#     img_mean = 0\n",
        "#     img_std = 0\n",
        "#   img_mean_list.append(img_mean)\n",
        "#   img_std_list.append(img_std)\n",
        "#   lable_list.append(df['lable'][i])\n",
        "#   landmarkls.append(data) \n",
        "\n",
        "# print(len(img_mean_list))\n",
        "# print(len(img_std_list))\n",
        "# print(len(lable_list))\n",
        "# print(len(landmarkls))\n",
        "# df_jaffe = pd.DataFrame({'meanf': img_mean_list,'stdf': img_std_list, 'label':lable_list})\n",
        "# landmark_df = pd.DataFrame(np.array(landmarkls))\n",
        "# df_jaffe = pd.concat([df_jaffe, landmark_df], axis=1, join=\"inner\")\n",
        "# df_jaffe.drop(df_jaffe[df_jaffe.meanf == 0].index, inplace=True)\n",
        "\n",
        "# from google.colab import files\n",
        "# df_jaffe.to_csv('df_jaffe.csv') \n",
        "# files.download('df_jaffe.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpdSMJ28O1z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNDGB73Cdlt9"
      },
      "source": [
        "### **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_kfFaaKZ14Y",
        "outputId": "aa6ae082-f0cb-4fd8-e2c8-89f0afed0167"
      },
      "source": [
        "dfck = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/feature/ckplus.csv')\n",
        "dfkaggle = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/feature/df_kaggle.csv')\n",
        "df_jaffe = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/feature/df_jaffe.csv')\n",
        "dfkaggle['meanf'] = dfkaggle['meanf']/255.0\n",
        "dfkaggle['stdf'] = dfkaggle['stdf']/255.0\n",
        "df_jaffe['meanf'] = df_jaffe['meanf']/255.0\n",
        "df_jaffe['stdf'] = df_jaffe['stdf']/255.0\n",
        "\n",
        "# result = dfck.append(dfkaggle, sort=False)\n",
        "# print(result.shape)\n",
        "\n",
        "df = pd.concat([dfck, dfkaggle, df_jaffe], ignore_index = True)\n",
        "df.reset_index()\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(23715, 271)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXEqZhEgpp8U"
      },
      "source": [
        "# sns.countplot(x = 'label',\n",
        "#               data = df,\n",
        "#               order = df.label.value_counts().index);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6swRUvcRtFU"
      },
      "source": [
        "X = df.drop('label', axis=1)\n",
        "y = df['label'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV5wI43hqlRW"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "from numpy import where\n",
        "\n",
        "sm = SMOTE(sampling_strategy='auto')\n",
        "X_sm, y_sm = sm.fit_resample(df.drop('label', axis=1), df['label'])\n",
        "\n",
        "# smote = SMOTE(ratio='minority')\n",
        "# X_sm, y_sm = smote.fit_sample(result.drop('label', axis=1), result['label'])\n",
        "\n",
        "# # summarize the new class distribution\n",
        "# counter = Counter(y)\n",
        "# print(counter)\n",
        "# # scatter plot of examples by class label\n",
        "# for label, _ in counter.items():\n",
        "# \trow_ix = where(y == label)[0]\n",
        "# \tpyplot.scatter(X_sm[row_ix, 0], X_sm[row_ix, 1], label=str(label))\n",
        "# pyplot.legend()\n",
        "# pyplot.show()\n",
        "\n",
        "# newdata = pd.DataFrame(y_sm)\n",
        "# sns.countplot(x = 0,\n",
        "#               data = newdata,\n",
        "#               order = newdata[0].value_counts().index);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krd2EPW_vVoO"
      },
      "source": [
        "### -- RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RteIiGvg_P0Y"
      },
      "source": [
        "# X = df.drop('label', axis=1)\n",
        "# y = df['label'] \n",
        "\n",
        "# # rf = RandomForestClassifier(n_estimators=1000, random_state=0).fit(X, y)\n",
        "# rf = RandomForestClassifier(n_estimators=1000, random_state=0, class_weight='balanced').fit(X, y)\n",
        "# joblib.dump(rf, \"rf_ck_kaggle_jaffe_balanced.joblib\")\n",
        "# !cp rf_ck_kaggle_jaffe_balanced.joblib drive/MyDrive/AIHealthcare/Emotion_Categ_from_Vdo_frame_img_SeqVoting/model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIxFboP4vYb8"
      },
      "source": [
        "### -- ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TumB0crYTBM0"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y3WLA7PPM9N"
      },
      "source": [
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "enc = enc.fit(np.array(y).reshape(-1,1))\n",
        "y_train = enc.transform(np.array(y_train).reshape(-1,1))\n",
        "y_test = enc.transform(np.array(y_test).reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f2ZVRQsvU0t"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_dim = 270))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(output_dim = 6, activation='softmax'))\n",
        "model.add(Dense(6, activation='softmax')) \n",
        "# model.summary()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "# model.predict(X.values[100].reshape(1,-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "098aP5Z6QgMs"
      },
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUVDe06aSBs0"
      },
      "source": [
        "model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwDHbty-UJ00"
      },
      "source": [
        "  plot_cm(\n",
        "  enc.inverse_transform(y_test),\n",
        "  enc.inverse_transform(y_pred),\n",
        "  enc.categories_[0]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX2Da1XBUsMz"
      },
      "source": [
        "### ---Vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g480zw9FDmGo"
      },
      "source": [
        "# X_train = df.drop('label', axis=1).values\n",
        "# y_train = df['label'].values\n",
        "\n",
        "# clf_poly_svm = SVC(kernel=\"poly\", random_state=11, probability=True, tol=1e-3)\n",
        "# clf_poly_svm.fit(X_train, y_train)\n",
        "# joblib.dump(clf_poly_svm, \"model/clf_poly_svm.joblib\")\n",
        "\n",
        "# clf_lin_svm = SVC(kernel='linear', random_state=7, probability=True, tol=1e-3)\n",
        "# clf_lin_svm.fit(X_train, y_train)\n",
        "# joblib.dump(clf_lin_svm, \"model/clf_lin_svm.joblib\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahv820-Jt9LZ"
      },
      "source": [
        "# scaler = MinMaxScaler()\n",
        "# sdss = scaler.fit_transform(df.drop('label', axis=1))\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(sdss, df['label'], test_size=0.2)\n",
        "\n",
        "# xgb = XGBClassifier(n_estimators=100)\n",
        "# xgb = xgb.fit(X_train, y_train)\n",
        "# preds = xgb.predict(X_test)\n",
        "# acc_xgb = (preds == y_test).sum().astype(float) / len(preds)*100\n",
        "# print(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\n",
        "# joblib.dump(xgb, \"xgb_more.joblib\")\n",
        "\n",
        "# xgboost = XGBClassifier(max_depth=5, learning_rate=0.01,\n",
        "#                         n_estimators=100, gamma=0, \n",
        "#                         min_child_weight=1, subsample=0.8,\n",
        "#                         colsample_bytree=0.8, reg_alpha=0.005)\n",
        "# xgboost = xgboost.fit(X_train, y_train)\n",
        "# preds = xgboost.predict(X_test)\n",
        "# accuracy = (preds == y_test).sum().astype(float) / len(preds)*100\n",
        "\n",
        "# print(\"XGBoost's prediction accuracy WITH optimal hyperparameters is: %3.2f\" % (accuracy))\n",
        "# joblib.dump(xgboost, \"xgb_opt_more.joblib\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtWhMGgH8ZCb"
      },
      "source": [
        "## load_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAldOReY8qjT"
      },
      "source": [
        "def load_model(model_name, model_type, model_path):\n",
        "  \n",
        "  if model_name == 'rf_more_6class':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')\n",
        "  elif model_name == 'knn_more_6class':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')\n",
        "  elif model_name == 'rf_fewer_6class':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')     \n",
        "  elif model_name == 'knn_fewer_6class':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')     \n",
        "  elif model_name == 'clf_lin_svm':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')  \n",
        "  elif model_name == 'clf_poly_svm':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib') \n",
        "  elif model_name in ['rf_feature_6class','rf_ck_kaggle', 'rf_ck_kaggle_jaffe']:\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')       \n",
        "  elif model_name == 'xgb_opt_more_6class':\n",
        "    model = joblib.load(str(model_path)+model_name+'.joblib')       \n",
        "  elif model_name == 'resmasknet':\n",
        "    model = ResMaskNet()\n",
        "  elif model_name == 'fernet':\n",
        "    model = ferNetModule()\n",
        "\n",
        "  elif model_name == 'svm':\n",
        "    pca_model = joblib.load(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/emo_hog_pca.joblib\")\n",
        "    classifier = joblib.load(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/emoSVM38.joblib\")\n",
        "    scaler = joblib.load(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/emo_hog_scalar.joblib\")\n",
        "    model = [pca_model, classifier, scaler]\n",
        "\n",
        "  elif model_name == 'vgg16':\n",
        "    model_VGG = VGG16(weights='imagenet', include_top=False)\n",
        "    VGG16_model = Sequential()\n",
        "    VGG16_model.add(GlobalAveragePooling2D(input_shape=(10, 10, 512)))\n",
        "    VGG16_model.add(Dense(6, activation='softmax'))\n",
        "    VGG16_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    VGG16_model.load_weights('drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/weights_best_VGG16_6c.hdf5')\n",
        "    model = [model_VGG, VGG16_model]\n",
        "    return model\n",
        "\n",
        "  elif model_name == 'xception':\n",
        "    model_Xception = Xception(weights='imagenet', include_top=False)\n",
        "    Xception_model = Sequential()\n",
        "    Xception_model.add(GlobalAveragePooling2D(input_shape=(11, 11, 2048)))\n",
        "    Xception_model.add(Dense(6, activation='softmax'))\n",
        "    Xception_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    Xception_model.load_weights('drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/weights_best._Xception6c.hdf5')\n",
        "    model = [model_Xception, Xception_model]\n",
        "    return model\n",
        "\n",
        "  elif model_name == 'aug_vgg':\n",
        "    vgg_aug = VGG16(include_top=False, weights='imagenet',input_shape=(350,350,3))\n",
        "    vgg_aug_output = vgg_aug.layers[-1].output\n",
        "    vgg_aug_model = Model(vgg_aug.input, vgg_aug_output)\n",
        "    model_vgg_aug = Sequential()\n",
        "    model_vgg_aug.add(vgg_aug_model)\n",
        "    model_vgg_aug.add(GlobalAveragePooling2D(input_shape=(10, 10, 512)))\n",
        "    model_vgg_aug.add(Dense(6, activation='sigmoid'))\n",
        "    model_vgg_aug.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    model_vgg_aug.load_weights('drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/weights_best_vgg_aug_6c.hdf5')\n",
        "    model = model_vgg_aug\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZQRS4d-8ZE6"
      },
      "source": [
        "## cal_mean_std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WmlPhRz8vtz"
      },
      "source": [
        "def cal_mean_std(image):\n",
        "  val = np.reshape(image[:,:,0], -1)\n",
        "  img_mean = np.mean(val)\n",
        "  img_std = np.std(val)\n",
        "  X = np.array([img_mean, img_std]).reshape(1, -1)\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbyi35mC8ZHD"
      },
      "source": [
        "## plot_confus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbx0Gka39I5c"
      },
      "source": [
        "def plot_confus(pred_lable, truelable):\n",
        "  print(metrics.classification_report(pred_lable, truelable.to_list()))\n",
        "  print('Accuracy: ',metrics.accuracy_score(truelable.to_list(), pred_lable))\n",
        "  mat = confusion_matrix(truelable.to_list(), pred_lable)\n",
        "  sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "  plt.xlabel('true label')\n",
        "  plt.ylabel('predicted label'); \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_cm(y_true, y_pred, class_names):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  fig, ax = plt.subplots(figsize=(9, 8)) \n",
        "  ax = sns.heatmap(\n",
        "      cm, \n",
        "      annot=True, \n",
        "      fmt=\"d\", \n",
        "      cmap=sns.diverging_palette(220, 20, n=7),\n",
        "      ax=ax\n",
        "  )\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  ax.set_xticklabels(class_names)\n",
        "  ax.set_yticklabels(class_names)\n",
        "  b, t = plt.ylim()\n",
        "  b += 0.5 \n",
        "  t -= 0.5\n",
        "  plt.ylim(b, t) \n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvhcNv9x8ZOK"
      },
      "source": [
        "## pyfeat_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWlHVH2Z9LxE"
      },
      "source": [
        "def pyfeat_model(model, img, data_name, model_name):\n",
        "\n",
        "  if data_name in ['ck+','jaffe']:\n",
        "    detectordlib = dlib.get_frontal_face_detector()\n",
        "    predictorlib = dlib.shape_predictor(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/shape_predictor_68_face_landmarks.dat\")\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detectordlib(gray)\n",
        "    landmark_listxy = []\n",
        "\n",
        "    for face in faces:\n",
        "      x1 = face.left()\n",
        "      y1 = face.top()\n",
        "      x2 = face.right()\n",
        "      y2 = face.bottom()\n",
        "      detected_faces1 = [[x1, y1, x2, y2, 1]]\n",
        "\n",
        "      if model_name in ['fernet','svm']:\n",
        "        landmarks = predictorlib(gray, face)\n",
        "        for n in range(0, 68):\n",
        "            x = landmarks.part(n).x\n",
        "            y = landmarks.part(n).y\n",
        "            temp = [x, y]\n",
        "            landmark_listxy.append(temp)\n",
        "        landmark_listxy = [np.array(landmark_listxy)] \n",
        "        if model_name == 'fernet':\n",
        "          landmarks = np.transpose(landmark_listxy)\n",
        "          if landmarks.shape[-1] == 68:\n",
        "              landmarks = convert68to49(landmarks)\n",
        "              landmarks = landmarks.T    \n",
        "\n",
        "        elif model_name == 'svm':\n",
        "          convex_hull, new_lands = extract_face(frame=img,detected_faces=detected_faces1,landmarks=landmark_listxy,size_output=112)\n",
        "          hogs = extract_hog(frame=convex_hull, visualize=False)\n",
        "          if len(hogs.shape) < 2:\n",
        "              hogs = hogs.reshape(1,-1)\n",
        "          if len(new_lands.shape) > 1:\n",
        "              new_lands = new_lands.flatten().reshape(1,-1)     \n",
        "          pca_transformed_frame = model[0].transform(model[2].fit_transform(hogs))\n",
        "          feature_cbd = np.concatenate((pca_transformed_frame, new_lands), 1)     \n",
        "          predict = []   \n",
        "          for keys in model[1]:\n",
        "              au_pred_con = model[1][keys].decision_function(feature_cbd)\n",
        "              au_pred_con = au_pred_con[0] \n",
        "              predict.append(au_pred_con)\n",
        "          predict = np.array(predict).reshape(1, -1)[0]                 \n",
        "\n",
        "  elif data_name == 'kaggle':\n",
        "    detectordlib = dlib.get_frontal_face_detector()\n",
        "    predictorlib = dlib.shape_predictor(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/shape_predictor_68_face_landmarks.dat\")\n",
        "    gray = img.astype('uint8')\n",
        "    img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
        "    detected_faces1 = [[0, 0, 48, 48, 1]]\n",
        "    landmark_listxy = []\n",
        "    faces = detectordlib(gray, 1)\n",
        "    facedlib = 'found'\n",
        "\n",
        "    if model_name == ('fernet' or 'svm'):\n",
        "      try:\n",
        "        landmarks = predictorlib(gray, faces[0])\n",
        "      except:\n",
        "        facedlib = 'not_found'\n",
        "        \n",
        "      if facedlib != 'not_found':\n",
        "        for n in range(0, 68):\n",
        "            x = landmarks.part(n).x\n",
        "            y = landmarks.part(n).y\n",
        "            temp = [x, y]\n",
        "            landmark_listxy.append(temp)\n",
        "        landmark_listxy = [np.array(landmark_listxy)] \n",
        "        landmarks = np.transpose(landmark_listxy)\n",
        "        if landmarks.shape[-1] == 68:\n",
        "            landmarks = convert68to49(landmarks)\n",
        "            landmarks = landmarks.T   \n",
        "\n",
        "      if model_name == 'svm':\n",
        "        convex_hull, new_lands = extract_face(frame=img,detected_faces=detected_faces1,landmarks=landmark_listxy,size_output=112)\n",
        "        hogs = extract_hog(frame=convex_hull, visualize=False)\n",
        "        if len(hogs.shape) < 2:\n",
        "            hogs = hogs.reshape(1,-1)\n",
        "        if len(new_lands.shape) > 1:\n",
        "            new_lands = new_lands.flatten().reshape(1,-1)     \n",
        "        pca_transformed_frame = model[0].transform(model[2].fit_transform(hogs))\n",
        "        feature_cbd = np.concatenate((pca_transformed_frame, new_lands), 1)     \n",
        "        predict = []   \n",
        "        for keys in model[1]:\n",
        "            au_pred_con = model[1][keys].decision_function(feature_cbd)\n",
        "            au_pred_con = au_pred_con[0] \n",
        "            predict.append(au_pred_con)\n",
        "        predict = np.array(predict).reshape(1, -1)[0]                 \n",
        "\n",
        "  if model_name == 'resmasknet':\n",
        "    predict = model.detect_emo(img,detected_faces1)[0]\n",
        "  elif model_name == 'fernet':\n",
        "    try:\n",
        "      predict = model.detect_emo(img,landmarks)[0]\n",
        "    except:\n",
        "      predict = [1, 0, 0, 0 ,0 ,0]\n",
        "  elif model_name == 'svm':\n",
        "    pass\n",
        "    \n",
        "  swapemo = [predict[0], predict[1], predict[2], predict[3], predict[4], predict[5]]\n",
        "  emo_arr = np.argmax(np.array(swapemo))\n",
        "  return emo_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVQkTIHy4F2n"
      },
      "source": [
        "## svm_linear_poly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae1SW15M4LMX"
      },
      "source": [
        "faceDet = cv2.CascadeClassifier(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/haarcascades/haarcascade_frontalface_default.xml\")\n",
        "faceDet_two = cv2.CascadeClassifier(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/haarcascades/haarcascade_frontalface_alt2.xml\")\n",
        "detector = dlib.get_frontal_face_detector() \n",
        "predictor = dlib.shape_predictor(\"drive/MyDrive/AIHealthcare/AIcare_Phrase1/model/shape_predictor_68_face_landmarks.dat\") \n",
        "\n",
        "def get_landmarks(image):\n",
        "    data={}\n",
        "    detections = detector(image, 1)\n",
        "    for k,d in enumerate(detections): #For all detected face instances individually\n",
        "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
        "        xlist = []\n",
        "        ylist = []\n",
        "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
        "            xlist.append(float(shape.part(i).x))\n",
        "            ylist.append(float(shape.part(i).y))\n",
        "        # calculate the center of gravity\n",
        "        xmean = np.mean(xlist)\n",
        "        ymean = np.mean(ylist)\n",
        "        # calculate the distance from center from both axis.\n",
        "        # this information is used to get the angle relative to center point.\n",
        "        xcentral = [(x-xmean) for x in xlist]\n",
        "        ycentral = [(y-ymean) for y in ylist]\n",
        "        landmarks_vectorised = []\n",
        "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
        "            # append x and y values\n",
        "            landmarks_vectorised.append(w)\n",
        "            landmarks_vectorised.append(z)\n",
        "            meannp = np.asarray((ymean,xmean))\n",
        "            coornp = np.asarray((z,w))\n",
        "            # calculate the euclidiean distance from center\n",
        "            dist = np.linalg.norm(coornp-meannp)\n",
        "            # append the distance to the feature vector\n",
        "            landmarks_vectorised.append(dist)\n",
        "            # apend the angle relative to the center of gravity.\n",
        "            landmarks_vectorised.append(math.degrees(math.atan2(y,x)))\n",
        "        data['landmarks_vectorised'] = landmarks_vectorised\n",
        "    if len(detections) < 1:\n",
        "        data['landmarks_vestorised'] = \"error\"\n",
        "    return data\n",
        "\n",
        "def detect_faces(img):\n",
        "  # print('img', img.shape)\n",
        "  face = faceDet.detectMultiScale(img, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "  face_two = faceDet_two.detectMultiScale(img, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "\n",
        "  if len(face) == 1:\n",
        "      facefeatures = face\n",
        "  elif len(face_two) == 1:\n",
        "      facefeatures = face_two\n",
        "  else:\n",
        "      facefeatures = \"\"\n",
        "\n",
        "  for (x, y, w, h) in facefeatures: \n",
        "      img = img[y:y+h, x:x+w] \n",
        "      try:\n",
        "          out = cv2.resize(img, (350, 350)) \n",
        "      except:\n",
        "          pass \n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnPrk6P1bZlc"
      },
      "source": [
        "## img_model_pred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucR06tyubda2"
      },
      "source": [
        "def img_model_pred(img, model, model_name):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img = detect_faces(gray)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    img = Image.fromarray(img)\n",
        "    img = img.resize((350, 350))\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    if model_name == 'vgg16':\n",
        "      x = model[0].predict(vgg_preprocess(x))\n",
        "    elif model_name == 'xception':\n",
        "      x = model[0].predict(xception_preprocess(x))\n",
        "    elif model_name == 'aug_vgg':\n",
        "      result = np.argmax(model.predict(x))\n",
        "    if model_name in ['vgg16', 'xception']:\n",
        "      result = np.argmax(model[1].predict(x))  \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwB4yl3R8ZTm"
      },
      "source": [
        "## evaluate_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YteuuK477l3"
      },
      "source": [
        "def evaluate_model(df, model, data_name, model_name): #df 1.image 2.label  \n",
        "  pred_lable = []\n",
        "  timefps = []\n",
        "  prev_frame_time = 0\n",
        "  new_frame_time = 0\n",
        "  \n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "\n",
        "    if model_name in ['rf_more_6class', 'knn_more_6class', 'rf_fewer_6class','knn_fewer_6class']:\n",
        "      img = df['image'][i].astype('float32')\n",
        "      image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.0\n",
        "      X = cal_mean_std(image)\n",
        "      ypred = model.predict(X)\n",
        "      pred_lable.append(ypred[0])\n",
        "\n",
        "    if model_name in ['xgb_more_6class', 'xgb_opt_more_6class', 'rf_feature_6class', 'rf_ck_kaggle', 'rf_ck_kaggle_jaffe']:\n",
        "      if data_name != 'kaggle':\n",
        "        face_image = detect_faces(df['image'][i])\n",
        "      else:\n",
        "        face_image = df['image'][i].astype('uint8')\n",
        "        \n",
        "      image = cv2.cvtColor(df['image'][i], cv2.COLOR_BGR2RGB)/255.0\n",
        "      X = cal_mean_std(image).tolist()[0]\n",
        "\n",
        "      try:\n",
        "        data = np.array(get_landmarks(face_image)['landmarks_vectorised']).tolist()\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      x_test = np.array(X+data).reshape(1,-1)\n",
        "      ypred = model.predict(x_test)\n",
        "      pred_lable.append(ypred[0])\n",
        "\n",
        "    elif model_name == 'resmasknet':\n",
        "      ypred = pyfeat_model(model, df['image'][i],data_name, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    elif model_name == 'fernet':\n",
        "      ypred = pyfeat_model(model, df['image'][i],data_name, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    elif model_name == 'svm':\n",
        "      ypred = pyfeat_model(model, df['image'][i],data_name, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    elif model_name in ['clf_lin_svm' , 'clf_poly_svm']:\n",
        "      img = df['image'][i].astype('uint8')\n",
        "      if data_name != 'kaggle':\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "      image = detect_faces(img)\n",
        "      data = np.array(get_landmarks(image)['landmarks_vectorised'])\n",
        "      emo_arr = model.predict_proba(data.reshape(1, -1))[0]\n",
        "      emo_arr = np.argmax(np.array(emo_arr))\n",
        "      pred_lable.append(emo_arr)\n",
        "    \n",
        "    elif model_name in 'vgg16':\n",
        "      ypred = img_model_pred(df['image'][i], model, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    elif model_name in 'xception':\n",
        "      ypred = img_model_pred(df['image'][i], model, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    elif model_name in 'aug_vgg':\n",
        "      ypred = img_model_pred(df['image'][i], model, model_name)\n",
        "      pred_lable.append(ypred)\n",
        "\n",
        "    new_frame_time = time.time()\n",
        "    fps = 1/(new_frame_time-prev_frame_time)\n",
        "    prev_frame_time = new_frame_time\n",
        "    timefps.append(fps)\n",
        "    \n",
        "  print('FPSav', (sum(timefps) / len(timefps)))  \n",
        "  pred_df = pd.DataFrame({'lable': pred_lable})\n",
        "  pred_df = convert_pred(pred_df, model_name)\n",
        "  plot_confus(pred_df, df['lable'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3T5KnNKql8b"
      },
      "source": [
        "# API Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616,
          "referenced_widgets": [
            "8167903362284e86aebf703c9fc869f9",
            "3fd2c769e86d40f3a3ff274b7cf3a4db",
            "2bb364f0f8894563877a7e43f01833e4",
            "c367b119b1d449af9dfb3cfd2b7892cd",
            "04a7934eac9c4ae493c73f9b1b99a1e9",
            "61de855b39274e639ba576571d544198",
            "d6d7a8e4d0554c208131e7ce71f94c68",
            "bcf9e46b030b469196c0a70b63a0c0cd"
          ]
        },
        "id": "PHyRo-qe65D9",
        "outputId": "1d873064-fca2-427e-a3a1-70c5c1f5e272"
      },
      "source": [
        "# dataset -> ['ck+', 'kaggle', 'jaffe']\n",
        "df = load_dataset(\"jaffe\")\n",
        "\n",
        "# model -> [rf_more_6class, knn_more_6class,\n",
        "#           rf_fewer_6class, knn_fewer_6class,\n",
        "#           clf_lin_svm, clf_poly_svm,\n",
        "#           resmasknet, fernet, svm,\n",
        "#           vgg16, xception, aug_vgg,\n",
        "#           rf_feature_6class, rf_ck_kaggle,\n",
        "#           rf_ck_kaggle_jaffe.joblib]\n",
        "\n",
        "model = load_model('resmasknet','joblib', 'drive/MyDrive/AIHealthcare/Emotion_Categ_from_Vdo_frame_img_SeqVoting/model/') #(model, model_type)\n",
        "\n",
        "# Evaluate\n",
        "evaluate_model(df, model, 'jaffe', 'resmasknet') # (df, model, dataset_name, model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert_class [3 4 5 0 1 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8167903362284e86aebf703c9fc869f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=212.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "FPSav 0.8164511115234566\n",
            "convert_pred [5 3 4 2 1 0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.75      0.18         4\n",
            "           1       0.17      0.78      0.28         9\n",
            "           2       0.25      0.11      0.15        19\n",
            "           3       0.42      0.85      0.56        26\n",
            "           4       0.77      0.33      0.46        92\n",
            "           5       0.61      0.40      0.49        62\n",
            "\n",
            "    accuracy                           0.42       212\n",
            "   macro avg       0.39      0.53      0.35       212\n",
            "weighted avg       0.59      0.42      0.44       212\n",
            "\n",
            "Accuracy:  0.419811320754717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6GFXbOcfUaJ"
      },
      "source": [
        "1. https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio\n",
        "2. https://github.com/yfliao/Emotion-Classification-Ravdess\n",
        "3. https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lW1-1VafUsX"
      },
      "source": [
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/jaffe_final.csv')\n",
        "# df.head(5)\n",
        "# jaffe = ['HAP','SAD','SUR','ANG','DIS','FEA'] \n",
        "# jaffe = [ 0,    1,     2,    3,    4,    5  ]\n",
        "# jaffe = [ 5,    6,     7,    1,    3,    4  ]\n",
        "# ck+  = [anger\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"]\n",
        "# ck+  = [ \"1\",      \"3\",      \"4\",    \"5\",      \"6\",        \"7\"   ] \n",
        "\n",
        "# df = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/jaffe.csv')\n",
        "# df.columns = ['happy','sadness','surprise','anger','disgust','fear']\n",
        "\n",
        "# array_jaffe = df[['HAP','SAD','SUR','ANG','DIS','FEA']].values\n",
        "# lable_list = []\n",
        "# for i in range(array_jaffe.shape[0]):\n",
        "#   lable = np.argmax(array_jaffe[i])\n",
        "#   lable_list.append(lable)\n",
        "\n",
        "# lable = pd.DataFrame(np.array(lable_list),columns=['lable'])\n",
        "# df_final = pd.concat([df, lable], axis=1, join=\"inner\")\n",
        "# from google.colab import files\n",
        "# df_final.to_csv('jaffe_final.csv') \n",
        "# files.download('jaffe_final.csv')\n",
        "\n",
        "# pathlist = []\n",
        "# import glob\n",
        "# for filepath in glob.iglob(r'drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/jaffedbase/*.tiff'):\n",
        "#   pathlist.append(filepath)\n",
        "#   # pathlist.append(filepath[64:70])\n",
        "\n",
        "# jaffe = pd.read_csv('drive/MyDrive/AIHealthcare/AIcare_Phrase1/data/JAFFE/JAFFE_df.csv')\n",
        "# jaffe['PIC'] = jaffe['PIC'].str.replace('-','.')\n",
        "# jaffe['path'] = 0\n",
        "\n",
        "# for i in range(jaffe.shape[0]):\n",
        "#   for j in range(len(pathlist)):\n",
        "#     if jaffe['PIC'][i] == pathlist[j][64:70]:\n",
        "#       jaffe['path'][i] = pathlist[j]\n",
        "#       # print('ok')\n",
        "# # jaffe\n",
        "\n",
        "# df = pd.concat([df_more_frame, landmark_df], axis=1, join=\"inner\")\n",
        "# from google.colab import files\n",
        "# jaffe.to_csv('jaffe.csv') \n",
        "# files.download('jaffe.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}